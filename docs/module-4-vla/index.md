# Module 4: Vision-Language-Action

This module introduces Vision-Language-Action (VLA) models for interpreting natural language commands and visual scenes, outputting high-level action plans for robotic systems.

## Learning Objectives

- Connect GPT-4 Vision to interpret natural language commands
- Process camera images and text together
- Generate structured action sequences
- Validate action feasibility with planners
- Handle ambiguous commands