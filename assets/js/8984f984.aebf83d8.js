"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[981],{6811:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>l});var t=r(4848),s=r(8453);const a={},o="Pose Estimation and TF Publishing",i={id:"module-3-perception/pose-tf",title:"Pose Estimation and TF Publishing",description:"Pose estimation determines the 3D position and orientation of objects. Publishing these poses to the TF (Transform) tree allows the robot to reason about spatial relationships and plan actions.",source:"@site/docs/module-3-perception/3-pose-tf.md",sourceDirName:"module-3-perception",slug:"/module-3-perception/pose-tf",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/pose-tf",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-3-perception/3-pose-tf.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{},sidebar:"docsSidebar",previous:{title:"Object Detection and Classification",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/object-detection"},next:{title:"RViz2 Visualization",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/rviz"}},c={},l=[{value:"Understanding TF2",id:"understanding-tf2",level:2},{value:"Key Concepts",id:"key-concepts",level:3},{value:"TF Tree Example",id:"tf-tree-example",level:3},{value:"Publishing Static Transforms",id:"publishing-static-transforms",level:2},{value:"Using Launch Files",id:"using-launch-files",level:3},{value:"Publishing Dynamic Transforms",id:"publishing-dynamic-transforms",level:2},{value:"Rotation Representations",id:"rotation-representations",level:2},{value:"Euler Angles to Quaternion",id:"euler-angles-to-quaternion",level:3},{value:"Quaternion to Euler Angles",id:"quaternion-to-euler-angles",level:3},{value:"Rotation Matrix to Quaternion",id:"rotation-matrix-to-quaternion",level:3},{value:"Object Pose Estimation",id:"object-pose-estimation",level:2},{value:"ArUco Marker Detection",id:"aruco-marker-detection",level:3},{value:"PnP (Perspective-n-Point) for Generic Objects",id:"pnp-perspective-n-point-for-generic-objects",level:3},{value:"Listening to Transforms",id:"listening-to-transforms",level:2},{value:"Complete Perception Pipeline",id:"complete-perception-pipeline",level:2},{value:"Visualization in RViz2",id:"visualization-in-rviz2",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Further Reading",id:"further-reading",level:2}];function m(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.h1,{id:"pose-estimation-and-tf-publishing",children:"Pose Estimation and TF Publishing"}),"\n",(0,t.jsx)(e.p,{children:"Pose estimation determines the 3D position and orientation of objects. Publishing these poses to the TF (Transform) tree allows the robot to reason about spatial relationships and plan actions."}),"\n",(0,t.jsx)(e.h2,{id:"understanding-tf2",children:"Understanding TF2"}),"\n",(0,t.jsx)(e.p,{children:"TF2 is ROS 2's coordinate frame transformation system."}),"\n",(0,t.jsx)(e.h3,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Frame"}),": A coordinate system (e.g., ",(0,t.jsx)(e.code,{children:"world"}),", ",(0,t.jsx)(e.code,{children:"camera_link"}),", ",(0,t.jsx)(e.code,{children:"base_link"}),")"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Transform"}),": Translation + rotation between two frames"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"TF Tree"}),": Hierarchical graph of frame relationships"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Static vs Dynamic"}),": Static transforms don't change; dynamic transforms update over time"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"tf-tree-example",children:"TF Tree Example"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"world\n\u251c\u2500\u2500 map\n\u2502   \u2514\u2500\u2500 odom\n\u2502       \u2514\u2500\u2500 base_link\n\u2502           \u251c\u2500\u2500 camera_link\n\u2502           \u2502   \u2514\u2500\u2500 camera_optical_frame\n\u2502           \u251c\u2500\u2500 left_gripper\n\u2502           \u2514\u2500\u2500 right_gripper\n\u2514\u2500\u2500 detected_object_1\n"})}),"\n",(0,t.jsx)(e.h2,{id:"publishing-static-transforms",children:"Publishing Static Transforms"}),"\n",(0,t.jsx)(e.p,{children:"For fixed relationships (e.g., camera mounted on robot)."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from rclpy.node import Node\nfrom tf2_ros import StaticTransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nimport math\n\nclass StaticTFPublisher(Node):\n    def __init__(self):\n        super().__init__('static_tf_publisher')\n\n        self.broadcaster = StaticTransformBroadcaster(self)\n\n        # Publish camera transform\n        self.publish_camera_transform()\n\n    def publish_camera_transform(self):\n        \"\"\"\n        Publish static transform from base_link to camera_link\n\n        Camera is 0.2m forward, 0.0m sideways, 0.5m up,\n        rotated 30 degrees down\n        \"\"\"\n        t = TransformStamped()\n\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'base_link'\n        t.child_frame_id = 'camera_link'\n\n        # Translation\n        t.transform.translation.x = 0.2\n        t.transform.translation.y = 0.0\n        t.transform.translation.z = 0.5\n\n        # Rotation (quaternion for 30-degree pitch)\n        pitch = math.radians(-30)\n        t.transform.rotation.x = math.sin(pitch / 2)\n        t.transform.rotation.y = 0.0\n        t.transform.rotation.z = 0.0\n        t.transform.rotation.w = math.cos(pitch / 2)\n\n        self.broadcaster.sendTransform(t)\n\ndef main():\n    rclpy.init()\n    node = StaticTFPublisher()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n"})}),"\n",(0,t.jsx)(e.h3,{id:"using-launch-files",children:"Using Launch Files"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        Node(\n            package='tf2_ros',\n            executable='static_transform_publisher',\n            arguments=['0.2', '0', '0.5', '0', '-0.5', '0', 'base_link', 'camera_link']\n            # Arguments: x y z yaw pitch roll parent_frame child_frame\n        )\n    ])\n"})}),"\n",(0,t.jsx)(e.h2,{id:"publishing-dynamic-transforms",children:"Publishing Dynamic Transforms"}),"\n",(0,t.jsx)(e.p,{children:"For moving objects or changing relationships."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nimport numpy as np\n\nclass DynamicTFPublisher(Node):\n    def __init__(self):\n        super().__init__('dynamic_tf_publisher')\n\n        self.broadcaster = TransformBroadcaster(self)\n\n        # Timer for periodic updates\n        self.timer = self.create_timer(0.1, self.publish_transforms)\n\n    def publish_transforms(self):\n        \"\"\"Publish dynamic transforms\"\"\"\n\n        # Example: Object detected at varying position\n        t = TransformStamped()\n\n        t.header.stamp = self.get_clock().now().to_msg()\n        t.header.frame_id = 'camera_link'\n        t.child_frame_id = 'detected_object'\n\n        # Position from perception system\n        t.transform.translation.x = 1.0\n        t.transform.translation.y = 0.5\n        t.transform.translation.z = 0.3\n\n        # Orientation (identity quaternion = no rotation)\n        t.transform.rotation.x = 0.0\n        t.transform.rotation.y = 0.0\n        t.transform.rotation.z = 0.0\n        t.transform.rotation.w = 1.0\n\n        self.broadcaster.sendTransform(t)\n"})}),"\n",(0,t.jsx)(e.h2,{id:"rotation-representations",children:"Rotation Representations"}),"\n",(0,t.jsx)(e.h3,{id:"euler-angles-to-quaternion",children:"Euler Angles to Quaternion"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def euler_to_quaternion(roll, pitch, yaw):\n    """\n    Convert Euler angles to quaternion\n\n    Args:\n        roll, pitch, yaw: Rotation angles in radians\n\n    Returns:\n        (x, y, z, w): Quaternion components\n    """\n    cy = np.cos(yaw * 0.5)\n    sy = np.sin(yaw * 0.5)\n    cp = np.cos(pitch * 0.5)\n    sp = np.sin(pitch * 0.5)\n    cr = np.cos(roll * 0.5)\n    sr = np.sin(roll * 0.5)\n\n    w = cr * cp * cy + sr * sp * sy\n    x = sr * cp * cy - cr * sp * sy\n    y = cr * sp * cy + sr * cp * sy\n    z = cr * cp * sy - sr * sp * cy\n\n    return (x, y, z, w)\n\n# Example usage\nroll, pitch, yaw = 0.0, np.radians(30), np.radians(45)\nquat = euler_to_quaternion(roll, pitch, yaw)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"quaternion-to-euler-angles",children:"Quaternion to Euler Angles"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def quaternion_to_euler(x, y, z, w):\n    """\n    Convert quaternion to Euler angles\n\n    Returns:\n        (roll, pitch, yaw): Rotation angles in radians\n    """\n    # Roll (x-axis rotation)\n    sinr_cosp = 2 * (w * x + y * z)\n    cosr_cosp = 1 - 2 * (x * x + y * y)\n    roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n    # Pitch (y-axis rotation)\n    sinp = 2 * (w * y - z * x)\n    if abs(sinp) >= 1:\n        pitch = np.copysign(np.pi / 2, sinp)\n    else:\n        pitch = np.arcsin(sinp)\n\n    # Yaw (z-axis rotation)\n    siny_cosp = 2 * (w * z + x * y)\n    cosy_cosp = 1 - 2 * (y * y + z * z)\n    yaw = np.arctan2(siny_cosp, cosy_cosp)\n\n    return (roll, pitch, yaw)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"rotation-matrix-to-quaternion",children:"Rotation Matrix to Quaternion"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def rotation_matrix_to_quaternion(R):\n    """\n    Convert 3x3 rotation matrix to quaternion\n\n    Args:\n        R: 3x3 rotation matrix\n\n    Returns:\n        (x, y, z, w): Quaternion\n    """\n    trace = np.trace(R)\n\n    if trace > 0:\n        s = 0.5 / np.sqrt(trace + 1.0)\n        w = 0.25 / s\n        x = (R[2, 1] - R[1, 2]) * s\n        y = (R[0, 2] - R[2, 0]) * s\n        z = (R[1, 0] - R[0, 1]) * s\n    elif (R[0, 0] > R[1, 1]) and (R[0, 0] > R[2, 2]):\n        s = 2.0 * np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2])\n        w = (R[2, 1] - R[1, 2]) / s\n        x = 0.25 * s\n        y = (R[0, 1] + R[1, 0]) / s\n        z = (R[0, 2] + R[2, 0]) / s\n    elif R[1, 1] > R[2, 2]:\n        s = 2.0 * np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2])\n        w = (R[0, 2] - R[2, 0]) / s\n        x = (R[0, 1] + R[1, 0]) / s\n        y = 0.25 * s\n        z = (R[1, 2] + R[2, 1]) / s\n    else:\n        s = 2.0 * np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1])\n        w = (R[1, 0] - R[0, 1]) / s\n        x = (R[0, 2] + R[2, 0]) / s\n        y = (R[1, 2] + R[2, 1]) / s\n        z = 0.25 * s\n\n    return (x, y, z, w)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"object-pose-estimation",children:"Object Pose Estimation"}),"\n",(0,t.jsx)(e.h3,{id:"aruco-marker-detection",children:"ArUco Marker Detection"}),"\n",(0,t.jsx)(e.p,{children:"ArUco markers provide known geometry for robust pose estimation."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import cv2\nimport cv2.aruco as aruco\n\nclass ArUcoPoseEstimator(Node):\n    def __init__(self):\n        super().__init__('aruco_pose_estimator')\n\n        # ArUco dictionary\n        self.aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_6X6_250)\n        self.aruco_params = aruco.DetectorParameters()\n\n        # Camera intrinsics (should load from camera_info in practice)\n        self.camera_matrix = np.array([\n            [615.0, 0.0, 320.0],\n            [0.0, 615.0, 240.0],\n            [0.0, 0.0, 1.0]\n        ])\n        self.dist_coeffs = np.zeros((5, 1))\n\n        # Marker size in meters\n        self.marker_size = 0.05  # 5cm\n\n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n        # Detect markers\n        corners, ids, rejected = aruco.detectMarkers(\n            gray, self.aruco_dict, parameters=self.aruco_params\n        )\n\n        if ids is not None:\n            # Estimate pose for each marker\n            rvecs, tvecs, _ = aruco.estimatePoseSingleMarkers(\n                corners, self.marker_size, self.camera_matrix, self.dist_coeffs\n            )\n\n            for i in range(len(ids)):\n                marker_id = ids[i][0]\n                rvec = rvecs[i][0]\n                tvec = tvecs[i][0]\n\n                # Publish TF\n                self.publish_marker_tf(marker_id, rvec, tvec, msg.header.stamp)\n\n                # Draw axis for visualization\n                cv2.drawFrameAxes(\n                    cv_image, self.camera_matrix, self.dist_coeffs,\n                    rvec, tvec, self.marker_size * 0.5\n                )\n\n            # Draw detected markers\n            aruco.drawDetectedMarkers(cv_image, corners, ids)\n\n        cv2.imshow(\"ArUco Detection\", cv_image)\n        cv2.waitKey(1)\n\n    def publish_marker_tf(self, marker_id, rvec, tvec, timestamp):\n        \"\"\"Publish marker pose as TF transform\"\"\"\n\n        t = TransformStamped()\n        t.header.stamp = timestamp\n        t.header.frame_id = 'camera_link'\n        t.child_frame_id = f'aruco_marker_{marker_id}'\n\n        # Translation\n        t.transform.translation.x = float(tvec[0])\n        t.transform.translation.y = float(tvec[1])\n        t.transform.translation.z = float(tvec[2])\n\n        # Convert rotation vector to quaternion\n        rotation_matrix, _ = cv2.Rodrigues(rvec)\n        quat = rotation_matrix_to_quaternion(rotation_matrix)\n\n        t.transform.rotation.x = quat[0]\n        t.transform.rotation.y = quat[1]\n        t.transform.rotation.z = quat[2]\n        t.transform.rotation.w = quat[3]\n\n        self.tf_broadcaster.sendTransform(t)\n"})}),"\n",(0,t.jsx)(e.h3,{id:"pnp-perspective-n-point-for-generic-objects",children:"PnP (Perspective-n-Point) for Generic Objects"}),"\n",(0,t.jsx)(e.p,{children:"For objects with known 3D models."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def estimate_pose_pnp(image_points, model_points, camera_matrix, dist_coeffs):\n    """\n    Estimate object pose using PnP\n\n    Args:\n        image_points: Nx2 array of 2D points in image\n        model_points: Nx3 array of corresponding 3D points in object frame\n        camera_matrix: 3x3 camera intrinsic matrix\n        dist_coeffs: Distortion coefficients\n\n    Returns:\n        rvec, tvec: Rotation and translation vectors\n    """\n    success, rvec, tvec = cv2.solvePnP(\n        model_points,\n        image_points,\n        camera_matrix,\n        dist_coeffs,\n        flags=cv2.SOLVEPNP_ITERATIVE\n    )\n\n    if not success:\n        return None, None\n\n    # Refine using Levenberg-Marquardt\n    rvec, tvec = cv2.solvePnPRefineLM(\n        model_points, image_points, camera_matrix, dist_coeffs, rvec, tvec\n    )\n\n    return rvec, tvec\n\n# Example: Cube with known corners\nmodel_points = np.array([\n    [0, 0, 0],      # Corner 1\n    [0.05, 0, 0],   # Corner 2\n    [0.05, 0.05, 0], # Corner 3\n    [0, 0.05, 0],   # Corner 4\n], dtype=np.float32)\n\n# Detected corners in image\nimage_points = np.array([\n    [100, 200],\n    [150, 210],\n    [140, 250],\n    [95, 245]\n], dtype=np.float32)\n\nrvec, tvec = estimate_pose_pnp(image_points, model_points, camera_matrix, dist_coeffs)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"listening-to-transforms",children:"Listening to Transforms"}),"\n",(0,t.jsx)(e.p,{children:"Lookup transforms to convert between frames."}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from tf2_ros import TransformListener, Buffer\nfrom tf2_geometry_msgs import do_transform_pose\nfrom geometry_msgs.msg import PoseStamped\n\nclass TFListener(Node):\n    def __init__(self):\n        super().__init__(\'tf_listener\')\n\n        # TF buffer and listener\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n    def get_transform(self, target_frame, source_frame):\n        """\n        Get transform from source_frame to target_frame\n\n        Returns:\n            TransformStamped or None if not available\n        """\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                target_frame,\n                source_frame,\n                rclpy.time.Time(),\n                timeout=rclpy.duration.Duration(seconds=1.0)\n            )\n            return transform\n        except Exception as e:\n            self.get_logger().error(f"Transform lookup failed: {e}")\n            return None\n\n    def transform_pose(self, pose_stamped, target_frame):\n        """\n        Transform pose to target frame\n\n        Args:\n            pose_stamped: PoseStamped message\n            target_frame: Target frame name\n\n        Returns:\n            Transformed PoseStamped or None\n        """\n        try:\n            transform = self.tf_buffer.lookup_transform(\n                target_frame,\n                pose_stamped.header.frame_id,\n                pose_stamped.header.stamp,\n                timeout=rclpy.duration.Duration(seconds=1.0)\n            )\n\n            transformed_pose = do_transform_pose(pose_stamped, transform)\n            return transformed_pose\n\n        except Exception as e:\n            self.get_logger().error(f"Transform failed: {e}")\n            return None\n\n# Example usage\ndef example_transform_usage(node):\n    # Create pose in camera frame\n    pose = PoseStamped()\n    pose.header.frame_id = \'camera_link\'\n    pose.header.stamp = node.get_clock().now().to_msg()\n    pose.pose.position.x = 1.0\n    pose.pose.position.y = 0.5\n    pose.pose.position.z = 0.3\n    pose.pose.orientation.w = 1.0\n\n    # Transform to base_link\n    transformed = node.transform_pose(pose, \'base_link\')\n\n    if transformed:\n        print(f"In base_link: x={transformed.pose.position.x:.2f}, "\n              f"y={transformed.pose.position.y:.2f}, "\n              f"z={transformed.pose.position.z:.2f}")\n'})}),"\n",(0,t.jsx)(e.h2,{id:"complete-perception-pipeline",children:"Complete Perception Pipeline"}),"\n",(0,t.jsx)(e.p,{children:"Integrate detection and pose estimation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class PerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__(\'perception_pipeline\')\n\n        # Object detector\n        self.detector = YOLODetector(\'yolov5s.onnx\')\n\n        # TF\n        self.tf_broadcaster = TransformBroadcaster(self)\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Camera intrinsics\n        self.camera_matrix = None\n        self.depth_image = None\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/color/image_raw\', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, \'/camera/depth/image_raw\', self.depth_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, \'/camera/color/camera_info\', self.camera_info_callback, 10\n        )\n\n        self.bridge = CvBridge()\n\n    def camera_info_callback(self, msg):\n        """Extract camera intrinsics"""\n        K = np.array(msg.k).reshape(3, 3)\n        self.camera_matrix = K\n\n    def depth_callback(self, msg):\n        """Store latest depth image"""\n        self.depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'passthrough\')\n\n    def rgb_callback(self, msg):\n        """Main perception loop"""\n        if self.camera_matrix is None or self.depth_image is None:\n            return\n\n        # Detect objects\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n        detections = self.detector.detect(cv_image)\n\n        # Estimate 3D poses\n        for i, det in enumerate(detections):\n            pose_3d = self.estimate_3d_pose(det, self.depth_image, self.camera_matrix)\n\n            if pose_3d:\n                # Publish TF\n                self.publish_object_tf(\n                    f"{det[\'class_name\']}_{i}",\n                    pose_3d,\n                    msg.header.stamp\n                )\n\n    def estimate_3d_pose(self, detection, depth_image, camera_matrix):\n        """Estimate 3D pose from 2D detection and depth"""\n        x, y, w, h = detection[\'box\']\n\n        cx = int(x + w / 2)\n        cy = int(y + h / 2)\n\n        # Get depth\n        roi = depth_image[int(y):int(y+h), int(x):int(x+w)]\n        roi_valid = roi[roi > 0]\n\n        if len(roi_valid) == 0:\n            return None\n\n        z = np.median(roi_valid) / 1000.0  # Convert mm to m\n\n        # Back-project\n        fx = camera_matrix[0, 0]\n        fy = camera_matrix[1, 1]\n        cx_cam = camera_matrix[0, 2]\n        cy_cam = camera_matrix[1, 2]\n\n        x_3d = (cx - cx_cam) * z / fx\n        y_3d = (cy - cy_cam) * z / fy\n\n        return (x_3d, y_3d, z)\n\n    def publish_object_tf(self, object_id, position, timestamp):\n        """Publish object pose to TF tree"""\n        t = TransformStamped()\n        t.header.stamp = timestamp\n        t.header.frame_id = \'camera_depth_optical_frame\'\n        t.child_frame_id = f\'object_{object_id}\'\n\n        t.transform.translation.x = position[0]\n        t.transform.translation.y = position[1]\n        t.transform.translation.z = position[2]\n\n        # Identity orientation (can be improved with full 6D pose estimation)\n        t.transform.rotation.w = 1.0\n\n        self.tf_broadcaster.sendTransform(t)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"visualization-in-rviz2",children:"Visualization in RViz2"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:"# Launch RViz2\nros2 run rviz2 rviz2\n\n# Add displays:\n# - TF: Shows all coordinate frames\n# - Camera: Shows camera feed\n# - Marker: Shows custom markers for objects\n"})}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(e.p,{children:"In the next lesson, you'll learn to visualize perception results in RViz2 and integrate with control systems."}),"\n",(0,t.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://docs.ros.org/en/humble/Tutorials/Intermediate/Tf2/Tf2-Main.html",children:"TF2 Tutorials"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://docs.opencv.org/master/dc/dbb/tutorial_py_calibration.html",children:"OpenCV Camera Calibration"})}),"\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://docs.opencv.org/master/d5/dae/tutorial_aruco_detection.html",children:"ArUco Documentation"})}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>i});var t=r(6540);const s={},a=t.createContext(s);function o(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);