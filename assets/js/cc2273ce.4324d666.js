"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[151],{6621:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>p,frontMatter:()=>i,metadata:()=>c,toc:()=>l});var o=t(4848),s=t(8453);const i={},r="Object Detection and Classification",c={id:"module-3-perception/object-detection",title:"Object Detection and Classification",description:"Object detection enables robots to identify and locate objects in their environment. This lesson covers traditional computer vision methods and modern deep learning approaches for object detection.",source:"@site/docs/module-3-perception/2-object-detection.md",sourceDirName:"module-3-perception",slug:"/module-3-perception/object-detection",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/object-detection",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-3-perception/2-object-detection.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"docsSidebar",previous:{title:"Vision and Depth Perception",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/vision-depth"},next:{title:"Pose Estimation and TF Publishing",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/pose-tf"}},a={},l=[{value:"Detection Approaches",id:"detection-approaches",level:2},{value:"1. Traditional Methods (Classical CV)",id:"1-traditional-methods-classical-cv",level:3},{value:"2. Deep Learning Methods (Modern)",id:"2-deep-learning-methods-modern",level:3},{value:"Color-Based Detection",id:"color-based-detection",level:2},{value:"Feature-Based Detection",id:"feature-based-detection",level:2},{value:"Deep Learning Object Detection",id:"deep-learning-object-detection",level:2},{value:"YOLO with ONNX Runtime",id:"yolo-with-onnx-runtime",level:3},{value:"ROS 2 Integration",id:"ros-2-integration",level:3},{value:"Instance Segmentation",id:"instance-segmentation",level:2},{value:"3D Object Detection",id:"3d-object-detection",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Model Quantization",id:"model-quantization",level:3},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"object-detection-and-classification",children:"Object Detection and Classification"}),"\n",(0,o.jsx)(n.p,{children:"Object detection enables robots to identify and locate objects in their environment. This lesson covers traditional computer vision methods and modern deep learning approaches for object detection."}),"\n",(0,o.jsx)(n.h2,{id:"detection-approaches",children:"Detection Approaches"}),"\n",(0,o.jsx)(n.h3,{id:"1-traditional-methods-classical-cv",children:"1. Traditional Methods (Classical CV)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Color-based segmentation"}),"\n",(0,o.jsx)(n.li,{children:"Template matching"}),"\n",(0,o.jsx)(n.li,{children:"Feature detection (SIFT, ORB, SURF)"}),"\n",(0,o.jsx)(n.li,{children:"Cascade classifiers"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"2-deep-learning-methods-modern",children:"2. Deep Learning Methods (Modern)"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"YOLO (You Only Look Once)"}),"\n",(0,o.jsx)(n.li,{children:"SSD (Single Shot Detector)"}),"\n",(0,o.jsx)(n.li,{children:"Faster R-CNN"}),"\n",(0,o.jsx)(n.li,{children:"EfficientDet"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"color-based-detection",children:"Color-Based Detection"}),"\n",(0,o.jsx)(n.p,{children:"Simple but effective for controlled environments."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\n\nclass ColorDetector(Node):\n    def __init__(self):\n        super().__init__('color_detector')\n\n        self.subscription = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n\n        self.detection_pub = self.create_publisher(\n            Image, '/detection/image', 10\n        )\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Detect red objects\n        detections = self.detect_color(cv_image, color='red')\n\n        # Draw bounding boxes\n        output_image = self.draw_detections(cv_image, detections)\n\n        # Publish result\n        output_msg = self.bridge.cv2_to_imgmsg(output_image, 'bgr8')\n        self.detection_pub.publish(output_msg)\n\n    def detect_color(self, image, color='red'):\n        \"\"\"\n        Detect objects of specific color\n\n        Returns:\n            List of bounding boxes [(x, y, w, h), ...]\n        \"\"\"\n        # Convert to HSV\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges\n        color_ranges = {\n            'red': [(0, 100, 100), (10, 255, 255)],  # Red wraps around\n            'red2': [(170, 100, 100), (180, 255, 255)],\n            'blue': [(100, 100, 100), (130, 255, 255)],\n            'green': [(40, 50, 50), (80, 255, 255)],\n            'yellow': [(20, 100, 100), (30, 255, 255)],\n        }\n\n        # Create mask\n        if color == 'red':\n            # Red needs two ranges\n            mask1 = cv2.inRange(hsv, np.array(color_ranges['red'][0]),\n                               np.array(color_ranges['red'][1]))\n            mask2 = cv2.inRange(hsv, np.array(color_ranges['red2'][0]),\n                               np.array(color_ranges['red2'][1]))\n            mask = cv2.bitwise_or(mask1, mask2)\n        else:\n            lower, upper = color_ranges[color]\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n\n        # Clean up mask\n        kernel = np.ones((5, 5), np.uint8)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n\n        # Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Get bounding boxes\n        detections = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n\n            if area > 500:  # Minimum area threshold\n                x, y, w, h = cv2.boundingRect(contour)\n                detections.append((x, y, w, h))\n\n        return detections\n\n    def draw_detections(self, image, detections):\n        \"\"\"Draw bounding boxes on image\"\"\"\n        output = image.copy()\n\n        for (x, y, w, h) in detections:\n            cv2.rectangle(output, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n            # Calculate center\n            cx = x + w // 2\n            cy = y + h // 2\n            cv2.circle(output, (cx, cy), 5, (0, 0, 255), -1)\n\n            # Add label\n            label = f\"Object ({w}x{h})\"\n            cv2.putText(output, label, (x, y-10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        return output\n"})}),"\n",(0,o.jsx)(n.h2,{id:"feature-based-detection",children:"Feature-Based Detection"}),"\n",(0,o.jsx)(n.p,{children:"Use SIFT/ORB for object recognition."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class FeatureDetector:\n    def __init__(self, template_image_path):\n        """\n        Initialize with template image to match\n\n        Args:\n            template_image_path: Path to reference image\n        """\n        self.template = cv2.imread(template_image_path)\n        self.template_gray = cv2.cvtColor(self.template, cv2.COLOR_BGR2GRAY)\n\n        # Initialize ORB detector (SIFT is patented, use ORB)\n        self.detector = cv2.ORB_create(nfeatures=1000)\n\n        # Detect template keypoints\n        self.template_kp, self.template_desc = self.detector.detectAndCompute(\n            self.template_gray, None\n        )\n\n        # Matcher\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n\n    def detect_object(self, scene_image, min_matches=10):\n        """\n        Detect template object in scene\n\n        Returns:\n            Homography matrix and corners if found, else None\n        """\n        scene_gray = cv2.cvtColor(scene_image, cv2.COLOR_BGR2GRAY)\n\n        # Detect scene keypoints\n        scene_kp, scene_desc = self.detector.detectAndCompute(scene_gray, None)\n\n        if scene_desc is None:\n            return None\n\n        # Match features\n        matches = self.matcher.knnMatch(self.template_desc, scene_desc, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for pair in matches:\n            if len(pair) == 2:\n                m, n = pair\n                if m.distance < 0.75 * n.distance:\n                    good_matches.append(m)\n\n        if len(good_matches) < min_matches:\n            return None\n\n        # Find homography\n        src_pts = np.float32([self.template_kp[m.queryIdx].pt for m in good_matches])\n        dst_pts = np.float32([scene_kp[m.trainIdx].pt for m in good_matches])\n\n        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n\n        if H is None:\n            return None\n\n        # Get template corners\n        h, w = self.template_gray.shape\n        template_corners = np.float32([\n            [0, 0],\n            [w, 0],\n            [w, h],\n            [0, h]\n        ]).reshape(-1, 1, 2)\n\n        # Transform to scene coordinates\n        scene_corners = cv2.perspectiveTransform(template_corners, H)\n\n        return H, scene_corners\n\n    def draw_detection(self, scene_image, detection):\n        """Draw detected object outline"""\n        if detection is None:\n            return scene_image\n\n        H, corners = detection\n        output = scene_image.copy()\n\n        # Draw outline\n        corners_int = np.int32(corners)\n        cv2.polylines(output, [corners_int], True, (0, 255, 0), 3)\n\n        return output\n'})}),"\n",(0,o.jsx)(n.h2,{id:"deep-learning-object-detection",children:"Deep Learning Object Detection"}),"\n",(0,o.jsx)(n.h3,{id:"yolo-with-onnx-runtime",children:"YOLO with ONNX Runtime"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import onnxruntime as ort\n\nclass YOLODetector:\n    def __init__(self, model_path, conf_threshold=0.5, iou_threshold=0.4):\n        \"\"\"\n        Initialize YOLO detector\n\n        Args:\n            model_path: Path to ONNX model file\n            conf_threshold: Confidence threshold\n            iou_threshold: IoU threshold for NMS\n        \"\"\"\n        self.session = ort.InferenceSession(model_path)\n        self.conf_threshold = conf_threshold\n        self.iou_threshold = iou_threshold\n\n        # Input shape\n        self.input_name = self.session.get_inputs()[0].name\n        self.input_shape = self.session.get_inputs()[0].shape\n\n        # COCO class names\n        self.classes = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n            'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n            'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n            'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n            'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n            'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n            'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n            'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n            'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n            'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n    def preprocess(self, image):\n        \"\"\"Preprocess image for YOLO\"\"\"\n        # Resize to input shape\n        input_h, input_w = self.input_shape[2:4]\n        resized = cv2.resize(image, (input_w, input_h))\n\n        # Normalize to [0, 1]\n        input_tensor = resized.astype(np.float32) / 255.0\n\n        # HWC to CHW\n        input_tensor = np.transpose(input_tensor, (2, 0, 1))\n\n        # Add batch dimension\n        input_tensor = np.expand_dims(input_tensor, axis=0)\n\n        return input_tensor\n\n    def postprocess(self, outputs, original_shape):\n        \"\"\"Post-process YOLO outputs\"\"\"\n        predictions = outputs[0][0]  # [num_boxes, 85] for COCO\n\n        boxes = []\n        scores = []\n        class_ids = []\n\n        img_h, img_w = original_shape[:2]\n        input_h, input_w = self.input_shape[2:4]\n\n        for prediction in predictions:\n            confidence = prediction[4]\n\n            if confidence > self.conf_threshold:\n                # Get class scores\n                class_scores = prediction[5:]\n                class_id = np.argmax(class_scores)\n                class_score = class_scores[class_id]\n\n                final_score = confidence * class_score\n\n                if final_score > self.conf_threshold:\n                    # Box coordinates (cx, cy, w, h) normalized\n                    cx, cy, w, h = prediction[0:4]\n\n                    # Convert to pixel coordinates\n                    cx = cx * img_w / input_w\n                    cy = cy * img_h / input_h\n                    w = w * img_w / input_w\n                    h = h * img_h / input_h\n\n                    # Convert to (x, y, w, h)\n                    x = cx - w / 2\n                    y = cy - h / 2\n\n                    boxes.append([x, y, w, h])\n                    scores.append(final_score)\n                    class_ids.append(class_id)\n\n        # Apply NMS\n        indices = cv2.dnn.NMSBoxes(boxes, scores, self.conf_threshold, self.iou_threshold)\n\n        detections = []\n        if len(indices) > 0:\n            for i in indices.flatten():\n                detections.append({\n                    'box': boxes[i],\n                    'score': scores[i],\n                    'class_id': class_ids[i],\n                    'class_name': self.classes[class_ids[i]]\n                })\n\n        return detections\n\n    def detect(self, image):\n        \"\"\"Run detection on image\"\"\"\n        # Preprocess\n        input_tensor = self.preprocess(image)\n\n        # Inference\n        outputs = self.session.run(None, {self.input_name: input_tensor})\n\n        # Postprocess\n        detections = self.postprocess(outputs, image.shape)\n\n        return detections\n\n    def draw_detections(self, image, detections):\n        \"\"\"Draw detections on image\"\"\"\n        output = image.copy()\n\n        for det in detections:\n            x, y, w, h = [int(v) for v in det['box']]\n            score = det['score']\n            class_name = det['class_name']\n\n            # Draw box\n            cv2.rectangle(output, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n            # Draw label\n            label = f\"{class_name}: {score:.2f}\"\n            label_size, baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n\n            # Background rectangle\n            cv2.rectangle(output,\n                         (x, y - label_size[1] - 10),\n                         (x + label_size[0], y),\n                         (0, 255, 0), -1)\n\n            # Text\n            cv2.putText(output, label, (x, y - 5),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n\n        return output\n"})}),"\n",(0,o.jsx)(n.h3,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"from vision_msgs.msg import Detection2DArray, Detection2D, ObjectHypothesisWithPose\n\nclass YOLONode(Node):\n    def __init__(self):\n        super().__init__('yolo_detector')\n\n        # Parameters\n        self.declare_parameter('model_path', 'yolov5s.onnx')\n        self.declare_parameter('confidence_threshold', 0.5)\n\n        model_path = self.get_parameter('model_path').value\n        conf_threshold = self.get_parameter('confidence_threshold').value\n\n        # Initialize detector\n        self.detector = YOLODetector(model_path, conf_threshold)\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, '/camera/image_raw', self.image_callback, 10\n        )\n\n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/detections', 10\n        )\n        self.image_pub = self.create_publisher(\n            Image, '/detection/image', 10\n        )\n\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n        # Detect\n        detections = self.detector.detect(cv_image)\n\n        # Publish detection messages\n        detection_msg = self.create_detection_msg(detections, msg.header)\n        self.detection_pub.publish(detection_msg)\n\n        # Publish annotated image\n        output_image = self.detector.draw_detections(cv_image, detections)\n        output_msg = self.bridge.cv2_to_imgmsg(output_image, 'bgr8')\n        self.image_pub.publish(output_msg)\n\n    def create_detection_msg(self, detections, header):\n        \"\"\"Convert detections to Detection2DArray message\"\"\"\n        msg = Detection2DArray()\n        msg.header = header\n\n        for det in detections:\n            detection = Detection2D()\n\n            # Bounding box\n            x, y, w, h = det['box']\n            detection.bbox.center.position.x = x + w / 2\n            detection.bbox.center.position.y = y + h / 2\n            detection.bbox.size_x = w\n            detection.bbox.size_y = h\n\n            # Hypothesis\n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.hypothesis.class_id = str(det['class_id'])\n            hypothesis.hypothesis.score = det['score']\n\n            detection.results.append(hypothesis)\n            detection.id = det['class_name']\n\n            msg.detections.append(detection)\n\n        return msg\n"})}),"\n",(0,o.jsx)(n.h2,{id:"instance-segmentation",children:"Instance Segmentation"}),"\n",(0,o.jsx)(n.p,{children:"For pixel-level object masks, use Mask R-CNN or similar."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class MaskRCNNDetector:\n    \"\"\"Simplified Mask R-CNN interface\"\"\"\n\n    def __init__(self, model_path):\n        import torch\n        from torchvision.models.detection import maskrcnn_resnet50_fpn\n\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        # Load model\n        self.model = maskrcnn_resnet50_fpn(pretrained=True)\n        self.model.to(self.device)\n        self.model.eval()\n\n    def detect(self, image):\n        \"\"\"Run instance segmentation\"\"\"\n        import torch\n        from torchvision import transforms\n\n        # Preprocess\n        transform = transforms.Compose([transforms.ToTensor()])\n        input_tensor = transform(image).unsqueeze(0).to(self.device)\n\n        # Inference\n        with torch.no_grad():\n            predictions = self.model(input_tensor)[0]\n\n        # Extract results\n        masks = predictions['masks'].cpu().numpy()\n        boxes = predictions['boxes'].cpu().numpy()\n        labels = predictions['labels'].cpu().numpy()\n        scores = predictions['scores'].cpu().numpy()\n\n        return {\n            'masks': masks,\n            'boxes': boxes,\n            'labels': labels,\n            'scores': scores\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"3d-object-detection",children:"3D Object Detection"}),"\n",(0,o.jsx)(n.p,{children:"Combine RGB and depth for 3D bounding boxes."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"def detect_3d_objects(rgb_image, depth_image, camera_intrinsics, detections_2d):\n    \"\"\"\n    Convert 2D detections to 3D bounding boxes\n\n    Args:\n        rgb_image: Color image\n        depth_image: Depth map (meters)\n        camera_intrinsics: CameraIntrinsics object\n        detections_2d: List of 2D detections\n\n    Returns:\n        List of 3D bounding boxes\n    \"\"\"\n    detections_3d = []\n\n    for det in detections_2d:\n        x, y, w, h = det['box']\n\n        # Get depth at center\n        cx = int(x + w / 2)\n        cy = int(y + h / 2)\n\n        # Median depth in box (more robust than single pixel)\n        roi = depth_image[int(y):int(y+h), int(x):int(x+w)]\n        roi_valid = roi[roi > 0]\n\n        if len(roi_valid) == 0:\n            continue\n\n        z = np.median(roi_valid)\n\n        # Back-project to 3D\n        fx, fy, cx_cam, cy_cam = camera_intrinsics.fx, camera_intrinsics.fy, \\\n                                  camera_intrinsics.cx, camera_intrinsics.cy\n\n        x_3d = (cx - cx_cam) * z / fx\n        y_3d = (cy - cy_cam) * z / fy\n        z_3d = z\n\n        # Estimate 3D size (rough approximation)\n        width_3d = w * z / fx\n        height_3d = h * z / fy\n        depth_3d = 0.2  # Assume fixed depth for simplicity\n\n        detections_3d.append({\n            'class_name': det['class_name'],\n            'score': det['score'],\n            'position': (x_3d, y_3d, z_3d),\n            'size': (width_3d, height_3d, depth_3d)\n        })\n\n    return detections_3d\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"model-quantization",children:"Model Quantization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Convert PyTorch model to ONNX with quantization\nimport torch\n\nmodel = ...  # Your trained model\ndummy_input = torch.randn(1, 3, 640, 640)\n\ntorch.onnx.export(\n    model,\n    dummy_input,\n    \"model_quantized.onnx\",\n    opset_version=13,\n    do_constant_folding=True,\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={\n        'input': {0: 'batch_size'},\n        'output': {0: 'batch_size'}\n    }\n)\n\n# Further quantize with ONNX Runtime\nfrom onnxruntime.quantization import quantize_dynamic\n\nquantize_dynamic(\"model.onnx\", \"model_int8.onnx\")\n"})}),"\n",(0,o.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Install ONNX Runtime with GPU support\npip install onnxruntime-gpu\n\n# Use CUDA execution provider\nsession = ort.InferenceSession(\n    model_path,\n    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n)\n"})}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsx)(n.p,{children:"In the next lesson, you'll learn how to estimate object poses and publish them to the TF tree."}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/ultralytics/yolov5",children:"YOLOv5 Documentation"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://docs.opencv.org/master/d2/d58/tutorial_table_of_content_dnn.html",children:"OpenCV DNN Module"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://github.com/ros-perception/vision_msgs",children:"vision_msgs ROS Package"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://onnxruntime.ai/",children:"ONNX Runtime"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>c});var o=t(6540);const s={},i=o.createContext(s);function r(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);