"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[858],{4647:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var o=t(4848),i=t(8453);const a={},s="Multi-Modal Processing",r={id:"module-4-vla/multimodal",title:"Multi-Modal Processing",description:"Process camera images and text together for enhanced robot understanding.",source:"@site/docs/module-4-vla/2-multimodal.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/multimodal",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/multimodal",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-4-vla/2-multimodal.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"docsSidebar",previous:{title:"GPT-4 Vision Integration",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/gpt4-vision"},next:{title:"Structured Action Generation",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/action-sequences"}},l={},c=[{value:"Multi-Modal Inputs",id:"multi-modal-inputs",level:2},{value:"Enhanced VLA with Context",id:"enhanced-vla-with-context",level:2},{value:"Multiple Image Processing",id:"multiple-image-processing",level:2},{value:"Depth Integration",id:"depth-integration",level:2},{value:"Object Grounding",id:"object-grounding",level:2},{value:"Temporal Reasoning",id:"temporal-reasoning",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"multi-modal-processing",children:"Multi-Modal Processing"}),"\n",(0,o.jsx)(n.p,{children:"Process camera images and text together for enhanced robot understanding."}),"\n",(0,o.jsx)(n.h2,{id:"multi-modal-inputs",children:"Multi-Modal Inputs"}),"\n",(0,o.jsx)(n.p,{children:"Combine:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Visual"}),": Camera images, depth maps"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Textual"}),": Commands, context, constraints"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Spatial"}),": Robot state, object poses"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Temporal"}),": Previous actions, history"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"enhanced-vla-with-context",children:"Enhanced VLA with Context"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ContextualVLA:\n    def __init__(self, api_key=None):\n        self.client = OpenAI(api_key=api_key)\n        self.conversation_history = []\n        self.scene_memory = []\n\n    def add_context(self, context_type, data):\n        """Add contextual information"""\n        self.conversation_history.append({\n            "type": context_type,\n            "data": data,\n            "timestamp": time.time()\n        })\n\n    def generate_actions_with_context(self, image, command, robot_state=None):\n        """\n        Generate actions using full context\n\n        Args:\n            image: Current scene image\n            command: User command\n            robot_state: Current robot configuration\n\n        Returns:\n            Action sequence with reasoning\n        """\n        # Build context prompt\n        context_parts = []\n\n        # Add robot state\n        if robot_state:\n            context_parts.append(f"Robot state: {json.dumps(robot_state)}")\n\n        # Add recent history\n        recent_history = self.conversation_history[-5:]\n        if recent_history:\n            history_str = "\\n".join([\n                f"- {h[\'type\']}: {h[\'data\']}"\n                for h in recent_history\n            ])\n            context_parts.append(f"Recent history:\\n{history_str}")\n\n        # Combine with command\n        full_prompt = f"""\n        Context:\n        {chr(10).join(context_parts)}\n\n        Current Command: {command}\n\n        Based on the image and context, generate appropriate robot actions.\n        Explain your reasoning.\n        """\n\n        # Encode image\n        image_base64 = self.encode_image(image)\n\n        response = self.client.chat.completions.create(\n            model="gpt-4-vision-preview",\n            messages=[\n                {"role": "system", "content": "You are a robot action planner with spatial reasoning."},\n                {\n                    "role": "user",\n                    "content": [\n                        {"type": "text", "text": full_prompt},\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{image_base64}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            max_tokens=800\n        )\n\n        result = response.choices[0].message.content\n\n        # Store in history\n        self.add_context("command", command)\n        self.add_context("response", result)\n\n        return result\n\n    def encode_image(self, image):\n        import base64, cv2\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        success, buffer = cv2.imencode(\'.jpg\', image_rgb)\n        return base64.b64encode(buffer).decode(\'utf-8\')\n'})}),"\n",(0,o.jsx)(n.h2,{id:"multiple-image-processing",children:"Multiple Image Processing"}),"\n",(0,o.jsx)(n.p,{children:"Process multiple views simultaneously."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def process_multiple_views(client, images, command):\n    """\n    Process multiple camera views\n\n    Args:\n        client: OpenAI client\n        images: List of OpenCV images\n        command: User command\n\n    Returns:\n        Integrated understanding\n    """\n    # Encode all images\n    encoded_images = [encode_image(img) for img in images]\n\n    # Build multi-view prompt\n    content = [\n        {\n            "type": "text",\n            "text": f"You have {len(images)} camera views. Command: {command}\\n\\n"\n                    "Analyze all views and provide a comprehensive action plan."\n        }\n    ]\n\n    # Add all images\n    for i, img_data in enumerate(encoded_images):\n        content.append({\n            "type": "text",\n            "text": f"\\n## View {i+1}:"\n        })\n        content.append({\n            "type": "image_url",\n            "image_url": {\n                "url": f"data:image/jpeg;base64,{img_data}"\n            }\n        })\n\n    response = client.chat.completions.create(\n        model="gpt-4-vision-preview",\n        messages=[{"role": "user", "content": content}],\n        max_tokens=1000\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,o.jsx)(n.h2,{id:"depth-integration",children:"Depth Integration"}),"\n",(0,o.jsx)(n.p,{children:"Include depth information in prompts."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def create_depth_visualization(depth_image):\n    """Create interpretable depth visualization"""\n    import cv2\n    import numpy as np\n\n    # Normalize depth\n    depth_normalized = cv2.normalize(depth_image, None, 0, 255, cv2.NORM_MINMAX)\n    depth_colored = cv2.applyColorMap(depth_normalized.astype(np.uint8), cv2.COLORMAP_JET)\n\n    # Add distance labels\n    h, w = depth_image.shape\n    for y in range(0, h, 100):\n        for x in range(0, w, 100):\n            dist = depth_image[y, x] / 1000.0  # Convert to meters\n            cv2.putText(depth_colored, f"{dist:.1f}m",\n                       (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n                       0.4, (255, 255, 255), 1)\n\n    return depth_colored\n\ndef process_with_depth(vla, rgb_image, depth_image, command):\n    """Process RGB and depth together"""\n\n    # Create depth visualization\n    depth_viz = create_depth_visualization(depth_image)\n\n    # Process both images\n    result = process_multiple_views(\n        vla.client,\n        [rgb_image, depth_viz],\n        f"{command} (Second image shows depth/distance information)"\n    )\n\n    return result\n'})}),"\n",(0,o.jsx)(n.h2,{id:"object-grounding",children:"Object Grounding"}),"\n",(0,o.jsx)(n.p,{children:"Link language to visual objects."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'def ground_objects_in_scene(vla, image, objects_of_interest):\n    """\n    Ground object references in scene\n\n    Args:\n        vla: VLA client\n        image: Scene image\n        objects_of_interest: List of object names\n\n    Returns:\n        Dict mapping object names to bounding boxes\n    """\n    prompt = f"""\n    Identify and locate these objects in the image: {\', \'.join(objects_of_interest)}.\n\n    For each object found, provide:\n    1. Object name\n    2. Bounding box (x, y, width, height) in pixels\n    3. Confidence (low/medium/high)\n\n    Format as JSON.\n    """\n\n    response = vla.client.chat.completions.create(\n        model="gpt-4-vision-preview",\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {"type": "text", "text": prompt},\n                    {\n                        "type": "image_url",\n                        "image_url": {\n                            "url": f"data:image/jpeg;base64,{vla.encode_image(image)}"\n                        }\n                    }\n                ]\n            }\n        ]\n    )\n\n    # Parse response (implement JSON extraction)\n    return parse_object_groundings(response.choices[0].message.content)\n'})}),"\n",(0,o.jsx)(n.h2,{id:"temporal-reasoning",children:"Temporal Reasoning"}),"\n",(0,o.jsx)(n.p,{children:"Include action history."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class TemporalVLA:\n    def __init__(self):\n        self.action_history = []\n        self.scene_history = []\n\n    def plan_next_action(self, current_image, goal):\n        """Plan considering previous actions"""\n\n        history_summary = self.summarize_history()\n\n        prompt = f"""\n        Goal: {goal}\n\n        Previous actions:\n        {history_summary}\n\n        Current scene: [see image]\n\n        What should the robot do next? Consider what has already been done.\n        """\n\n        # Generate with context\n        # ... (similar to previous examples)\n\n    def summarize_history(self):\n        """Create concise history summary"""\n        if not self.action_history:\n            return "No previous actions."\n\n        summary = "\\n".join([\n            f"{i+1}. {action[\'type\']}: {action.get(\'object\', \'N/A\')}"\n            for i, action in enumerate(self.action_history[-10:])\n        ])\n\n        return summary\n'})}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2306.13549",children:"Multi-Modal Learning"})}),"\n",(0,o.jsx)(n.li,{children:(0,o.jsx)(n.a,{href:"https://arxiv.org/abs/2304.10592",children:"Vision-Language Models"})}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);