"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[843],{6162:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var a=t(4848),i=t(8453);const o={},s="Voice Command Interface",r={id:"module-5-capstone/voice-interface",title:"Voice Command Interface",description:"Enable natural voice interaction with your humanoid robot using speech-to-text and text-to-speech.",source:"@site/docs/module-5-capstone/2-voice-interface.md",sourceDirName:"module-5-capstone",slug:"/module-5-capstone/voice-interface",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-5-capstone/voice-interface",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-5-capstone/2-voice-interface.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"docsSidebar",previous:{title:"System Integration",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-5-capstone/integration"},next:{title:"Final Demo and Presentation",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-5-capstone/final-demo"}},c={},d=[{value:"Speech-to-Text Options",id:"speech-to-text-options",level:2},{value:"Option 1: OpenAI Whisper (Recommended)",id:"option-1-openai-whisper-recommended",level:3},{value:"Option 2: Google Speech Recognition",id:"option-2-google-speech-recognition",level:3},{value:"ROS 2 Voice Node",id:"ros-2-voice-node",level:2},{value:"Wake Word Detection",id:"wake-word-detection",level:2},{value:"Text-to-Speech Feedback",id:"text-to-speech-feedback",level:2},{value:"Command Grammar",id:"command-grammar",level:2},{value:"Complete Voice Pipeline",id:"complete-voice-pipeline",level:2},{value:"Testing",id:"testing",level:2},{value:"Further Reading",id:"further-reading",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"voice-command-interface",children:"Voice Command Interface"}),"\n",(0,a.jsx)(n.p,{children:"Enable natural voice interaction with your humanoid robot using speech-to-text and text-to-speech."}),"\n",(0,a.jsx)(n.h2,{id:"speech-to-text-options",children:"Speech-to-Text Options"}),"\n",(0,a.jsx)(n.h3,{id:"option-1-openai-whisper-recommended",children:"Option 1: OpenAI Whisper (Recommended)"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Advantages"}),": Highly accurate, multilingual, open-source"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import whisper\nimport pyaudio\nimport wave\nimport numpy as np\n\nclass WhisperSTT:\n    def __init__(self, model_size="base"):\n        """\n        Initialize Whisper STT\n\n        Args:\n            model_size: tiny, base, small, medium, large\n        """\n        self.model = whisper.load_model(model_size)\n\n    def transcribe_file(self, audio_file):\n        """Transcribe audio file"""\n        result = self.model.transcribe(audio_file)\n        return result[\'text\']\n\n    def transcribe_audio_data(self, audio_data, sample_rate=16000):\n        """Transcribe numpy audio data"""\n        # Save temporary file\n        import tempfile\n        with tempfile.NamedTemporaryFile(suffix=\'.wav\', delete=False) as f:\n            self.save_audio(audio_data, f.name, sample_rate)\n            text = self.transcribe_file(f.name)\n\n        return text\n\n    def save_audio(self, data, filename, sample_rate):\n        """Save audio data to WAV file"""\n        import scipy.io.wavfile as wavfile\n        wavfile.write(filename, sample_rate, data)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"option-2-google-speech-recognition",children:"Option 2: Google Speech Recognition"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install SpeechRecognition\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import speech_recognition as sr\n\nclass GoogleSTT:\n    def __init__(self):\n        self.recognizer = sr.Recognizer()\n\n    def listen_and_transcribe(self):\n        """Listen to microphone and transcribe"""\n        with sr.Microphone() as source:\n            print("Listening...")\n            self.recognizer.adjust_for_ambient_noise(source)\n            audio = self.recognizer.listen(source)\n\n        try:\n            text = self.recognizer.recognize_google(audio)\n            return text\n        except sr.UnknownValueError:\n            return None\n        except sr.RequestError as e:\n            print(f"Error: {e}")\n            return None\n'})}),"\n",(0,a.jsx)(n.h2,{id:"ros-2-voice-node",children:"ROS 2 Voice Node"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from std_msgs.msg import String\nimport threading\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n\n        # STT engine\n        self.stt = WhisperSTT(model_size="base")\n\n        # Publisher\n        self.command_pub = self.create_publisher(\n            String, \'/voice_command\', 10\n        )\n\n        # Audio capture\n        self.audio_buffer = []\n        self.is_recording = False\n\n        # Start listening thread\n        self.listen_thread = threading.Thread(target=self.listen_loop)\n        self.listen_thread.daemon = True\n        self.listen_thread.start()\n\n    def listen_loop(self):\n        """Continuously listen for voice commands"""\n        import pyaudio\n\n        CHUNK = 1024\n        FORMAT = pyaudio.paInt16\n        CHANNELS = 1\n        RATE = 16000\n\n        p = pyaudio.PyAudio()\n\n        stream = p.open(\n            format=FORMAT,\n            channels=CHANNELS,\n            rate=RATE,\n            input=True,\n            frames_per_buffer=CHUNK\n        )\n\n        print("Voice interface ready. Say \'robot\' to start command...")\n\n        while rclpy.ok():\n            # Read audio chunk\n            data = stream.read(CHUNK)\n            audio_chunk = np.frombuffer(data, dtype=np.int16)\n\n            # Detect speech (simple energy-based)\n            if self.detect_speech(audio_chunk):\n                self.record_command(stream, RATE)\n\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n\n    def detect_speech(self, audio_chunk, threshold=500):\n        """Simple voice activity detection"""\n        energy = np.abs(audio_chunk).mean()\n        return energy > threshold\n\n    def record_command(self, stream, rate, max_duration=5):\n        """Record voice command"""\n        print("Recording...")\n\n        frames = []\n        for i in range(0, int(rate / 1024 * max_duration)):\n            data = stream.read(1024)\n            frames.append(np.frombuffer(data, dtype=np.int16))\n\n            # Stop if silence detected\n            if not self.detect_speech(frames[-1]):\n                break\n\n        # Convert to numpy array\n        audio_data = np.concatenate(frames)\n\n        # Transcribe\n        text = self.stt.transcribe_audio_data(audio_data, rate)\n\n        if text:\n            print(f"Recognized: {text}")\n\n            # Publish\n            msg = String()\n            msg.data = text\n            self.command_pub.publish(msg)\n        else:\n            print("No speech detected")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"wake-word-detection",children:"Wake Word Detection"}),"\n",(0,a.jsx)(n.p,{children:"Add wake word to activate listening."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import pv_porcupine\n\nclass WakeWordDetector:\n    def __init__(self, wake_word="robot"):\n        """\n        Initialize wake word detector\n\n        Requires Porcupine API key\n        """\n        self.porcupine = pvporcupine.create(\n            keywords=[wake_word]\n        )\n\n    def detect_wake_word(self, audio_frame):\n        """\n        Check if wake word is in audio\n\n        Args:\n            audio_frame: 16-bit PCM audio frame\n\n        Returns:\n            True if wake word detected\n        """\n        keyword_index = self.porcupine.process(audio_frame)\n        return keyword_index >= 0\n'})}),"\n",(0,a.jsx)(n.h2,{id:"text-to-speech-feedback",children:"Text-to-Speech Feedback"}),"\n",(0,a.jsx)(n.p,{children:"Provide voice feedback to user."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from gtts import gTTS\nimport os\n\nclass TextToSpeech:\n    def __init__(self):\n        pass\n\n    def speak(self, text):\n        """Convert text to speech and play"""\n        tts = gTTS(text=text, lang=\'en\')\n        tts.save("response.mp3")\n\n        # Play audio\n        os.system("mpg123 response.mp3")\n\nclass TTSNode(Node):\n    def __init__(self):\n        super().__init__(\'tts_node\')\n\n        self.tts = TextToSpeech()\n\n        # Subscribe to feedback messages\n        self.feedback_sub = self.create_subscription(\n            String, \'/robot_feedback\', self.feedback_callback, 10\n        )\n\n    def feedback_callback(self, msg):\n        """Speak feedback message"""\n        self.tts.speak(msg.data)\n'})}),"\n",(0,a.jsx)(n.h2,{id:"command-grammar",children:"Command Grammar"}),"\n",(0,a.jsx)(n.p,{children:"Define command patterns for better recognition."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'COMMAND_PATTERNS = {\n    "pick_up": r"pick up (the )?(.*)",\n    "place": r"place (it )?(on|in|at) (the )?(.*)",\n    "move_to": r"move to (the )?(.*)",\n    "look_at": r"look at (the )?(.*)",\n    "give_me": r"give me (the )?(.*)"\n}\n\nimport re\n\ndef parse_command(text):\n    """Parse natural language command"""\n    text = text.lower().strip()\n\n    for intent, pattern in COMMAND_PATTERNS.items():\n        match = re.match(pattern, text)\n        if match:\n            return {\n                \'intent\': intent,\n                \'entities\': match.groups()\n            }\n\n    return {\'intent\': \'unknown\', \'entities\': []}\n\n# Example\nresult = parse_command("Pick up the red cup")\n# {\'intent\': \'pick_up\', \'entities\': (\'the \', \'red cup\')}\n'})}),"\n",(0,a.jsx)(n.h2,{id:"complete-voice-pipeline",children:"Complete Voice Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class VoiceRobotInterface(Node):\n    def __init__(self):\n        super().__init__(\'voice_robot_interface\')\n\n        # Components\n        self.stt = WhisperSTT()\n        self.tts = TextToSpeech()\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, \'/task_command\', 10)\n\n        # Subscribers\n        self.status_sub = self.create_subscription(\n            String, \'/task_status\', self.status_callback, 10\n        )\n\n        self.current_status = "idle"\n\n    def process_voice_input(self, audio_data):\n        """Complete pipeline: voice \u2192 text \u2192 command"""\n\n        # 1. Speech to text\n        text = self.stt.transcribe_audio_data(audio_data)\n\n        if not text:\n            self.tts.speak("I didn\'t catch that. Please repeat.")\n            return\n\n        self.get_logger().info(f"Heard: {text}")\n        self.tts.speak(f"You said: {text}")\n\n        # 2. Parse command\n        parsed = parse_command(text)\n\n        if parsed[\'intent\'] == \'unknown\':\n            self.tts.speak("I don\'t understand that command.")\n            return\n\n        # 3. Publish command\n        msg = String()\n        msg.data = text\n        self.command_pub.publish(msg)\n\n        self.tts.speak("Understood. Executing now.")\n\n    def status_callback(self, msg):\n        """React to task status"""\n        status = msg.data\n\n        if status == "COMPLETED" and self.current_status != "COMPLETED":\n            self.tts.speak("Task completed successfully.")\n\n        elif status == "FAILED" and self.current_status != "FAILED":\n            self.tts.speak("Task failed. Please try again.")\n\n        self.current_status = status\n'})}),"\n",(0,a.jsx)(n.h2,{id:"testing",children:"Testing"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Test voice node\nros2 run voice_pkg voice_command_node\n\n# Say: "Robot, pick up the red cup"\n\n# Monitor published commands\nros2 topic echo /voice_command\n'})}),"\n",(0,a.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"Whisper Documentation"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"https://pypi.org/project/SpeechRecognition/",children:"SpeechRecognition Library"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var a=t(6540);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);