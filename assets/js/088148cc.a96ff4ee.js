"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[940],{905:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>_,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var r=t(4848),o=t(8453);const s={},i="Control System Integration",a={id:"module-3-perception/control-integration",title:"Control System Integration",description:"Integrating perception with control systems allows robots to react to their environment. This lesson covers perception-control loops, visual servoing, and reactive behaviors.",source:"@site/docs/module-3-perception/5-control-integration.md",sourceDirName:"module-3-perception",slug:"/module-3-perception/control-integration",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/control-integration",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-3-perception/5-control-integration.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{},sidebar:"docsSidebar",previous:{title:"RViz2 Visualization",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/rviz"},next:{title:"Module 3 Exercises",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/exercises"}},l={},c=[{value:"Perception-Control Architecture",id:"perception-control-architecture",level:2},{value:"Basic Reactive Controller",id:"basic-reactive-controller",level:2},{value:"Visual Servoing",id:"visual-servoing",level:2},{value:"State Machine for Complex Behaviors",id:"state-machine-for-complex-behaviors",level:2},{value:"Obstacle Avoidance",id:"obstacle-avoidance",level:2},{value:"Performance Monitoring",id:"performance-monitoring",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h1,{id:"control-system-integration",children:"Control System Integration"}),"\n",(0,r.jsx)(n.p,{children:"Integrating perception with control systems allows robots to react to their environment. This lesson covers perception-control loops, visual servoing, and reactive behaviors."}),"\n",(0,r.jsx)(n.h2,{id:"perception-control-architecture",children:"Perception-Control Architecture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Sensors \u2192 Perception \u2192 Decision Making \u2192 Motion Planning \u2192 Control \u2192 Actuators\n   \u2191                                                                      \u2193\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Feedback Loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,r.jsx)(n.h2,{id:"basic-reactive-controller",children:"Basic Reactive Controller"}),"\n",(0,r.jsx)(n.p,{children:"React to detected objects."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from geometry_msgs.msg import Twist\nfrom vision_msgs.msg import Detection2DArray\n\nclass ReactiveController(Node):\n    def __init__(self):\n        super().__init__(\'reactive_controller\')\n\n        # Subscribe to detections\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, \'/detections\', self.detection_callback, 10\n        )\n\n        # Publish velocity commands\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # State\n        self.target_detected = False\n        self.target_center_x = 0\n\n    def detection_callback(self, msg):\n        """React to detected objects"""\n\n        # Find target class (e.g., "person")\n        target_class = "person"\n        target_found = False\n\n        for detection in msg.detections:\n            if detection.id == target_class:\n                target_found = True\n                self.target_center_x = detection.bbox.center.position.x\n                break\n\n        self.target_detected = target_found\n\n        # Generate control command\n        self.generate_command()\n\n    def generate_command(self):\n        """Generate velocity command based on perception"""\n        cmd = Twist()\n\n        if self.target_detected:\n            # Center target in image (assume 640px width)\n            image_center = 320\n            error = self.target_center_x - image_center\n\n            # Proportional control\n            Kp = 0.001  # Tuning parameter\n            angular_velocity = -Kp * error\n\n            cmd.linear.x = 0.2  # Move forward\n            cmd.angular.z = angular_velocity\n        else:\n            # Search behavior\n            cmd.angular.z = 0.3  # Rotate to find target\n\n        self.cmd_vel_pub.publish(cmd)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"visual-servoing",children:"Visual Servoing"}),"\n",(0,r.jsx)(n.p,{children:"Control robot based on visual features."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class VisualServo(Node):\n    def __init__(self):\n        super().__init__(\'visual_servo\')\n\n        # Subscribe to ArUco marker poses\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Joint controller\n        self.joint_cmd_pub = self.create_publisher(\n            Float64MultiArray, \'/joint_position_controller/commands\', 10\n        )\n\n        # Control timer\n        self.timer = self.create_timer(0.1, self.control_loop)\n\n        # Target\n        self.target_frame = \'aruco_marker_0\'\n\n    def control_loop(self):\n        """Visual servoing control loop"""\n\n        try:\n            # Get marker pose in camera frame\n            transform = self.tf_buffer.lookup_transform(\n                \'camera_link\',\n                self.target_frame,\n                rclpy.time.Time()\n            )\n\n            # Extract position\n            x = transform.transform.translation.x\n            y = transform.transform.translation.y\n            z = transform.transform.translation.z\n\n            # Desired position\n            x_desired = 0.0  # Centered in camera\n            y_desired = 0.0\n            z_desired = 0.5  # 0.5m away\n\n            # Compute error\n            error_x = x_desired - x\n            error_y = y_desired - y\n            error_z = z_desired - z\n\n            # Proportional control\n            Kp = 0.5\n            vel_x = Kp * error_x\n            vel_y = Kp * error_y\n            vel_z = Kp * error_z\n\n            # Convert to joint velocities (inverse kinematics)\n            joint_velocities = self.velocity_ik(vel_x, vel_y, vel_z)\n\n            # Publish\n            cmd = Float64MultiArray()\n            cmd.data = joint_velocities\n            self.joint_cmd_pub.publish(cmd)\n\n        except Exception as e:\n            self.get_logger().warn(f"No transform: {e}")\n\n    def velocity_ik(self, vx, vy, vz):\n        """\n        Simplified velocity inverse kinematics\n\n        In practice, use actual Jacobian matrix\n        """\n        # Placeholder - implement actual IK\n        return [vx, vy, vz, 0.0, 0.0, 0.0]\n'})}),"\n",(0,r.jsx)(n.h2,{id:"state-machine-for-complex-behaviors",children:"State Machine for Complex Behaviors"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from enum import Enum\n\nclass State(Enum):\n    SEARCH = 1\n    APPROACH = 2\n    GRASP = 3\n    RETREAT = 4\n\nclass StateMachineController(Node):\n    def __init__(self):\n        super().__init__(\'state_machine_controller\')\n\n        self.state = State.SEARCH\n\n        # Subscribers\n        self.detection_sub = self.create_subscription(\n            Detection2DArray, \'/detections\', self.detection_callback, 10\n        )\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        # Timer\n        self.timer = self.create_timer(0.1, self.state_machine_update)\n\n        # State variables\n        self.target_detected = False\n        self.target_distance = 0.0\n\n    def detection_callback(self, msg):\n        # Update state variables based on detections\n        self.target_detected = len(msg.detections) > 0\n        if self.target_detected:\n            # Estimate distance from bbox size (simplified)\n            bbox_height = msg.detections[0].bbox.size_y\n            self.target_distance = 1000.0 / bbox_height  # Rough approximation\n\n    def state_machine_update(self):\n        """Main state machine logic"""\n\n        if self.state == State.SEARCH:\n            self.search_behavior()\n\n        elif self.state == State.APPROACH:\n            self.approach_behavior()\n\n        elif self.state == State.GRASP:\n            self.grasp_behavior()\n\n        elif self.state == State.RETREAT:\n            self.retreat_behavior()\n\n    def search_behavior(self):\n        """Rotate to find target"""\n        if self.target_detected:\n            self.state = State.APPROACH\n            self.get_logger().info("Target found, approaching")\n        else:\n            cmd = Twist()\n            cmd.angular.z = 0.5\n            self.cmd_vel_pub.publish(cmd)\n\n    def approach_behavior(self):\n        """Move toward target"""\n        if not self.target_detected:\n            self.state = State.SEARCH\n            self.get_logger().info("Lost target, searching")\n        elif self.target_distance < 0.3:\n            self.state = State.GRASP\n            self.get_logger().info("Reached target, grasping")\n        else:\n            cmd = Twist()\n            cmd.linear.x = 0.2\n            self.cmd_vel_pub.publish(cmd)\n\n    def grasp_behavior(self):\n        """Execute grasp"""\n        # Trigger gripper close\n        self.get_logger().info("Grasping")\n        # ... gripper control logic ...\n\n        # After grasp completes\n        self.state = State.RETREAT\n\n    def retreat_behavior(self):\n        """Move back after grasp"""\n        cmd = Twist()\n        cmd.linear.x = -0.1\n        self.cmd_vel_pub.publish(cmd)\n\n        # After retreating\n        # self.state = State.SEARCH\n'})}),"\n",(0,r.jsx)(n.h2,{id:"obstacle-avoidance",children:"Obstacle Avoidance"}),"\n",(0,r.jsx)(n.p,{children:"Use depth sensor for collision avoidance."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class ObstacleAvoidance(Node):\n    def __init__(self):\n        super().__init__(\'obstacle_avoidance\')\n\n        # Subscribe to depth or laser scan\n        self.scan_sub = self.create_subscription(\n            LaserScan, \'/scan\', self.scan_callback, 10\n        )\n\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\n\n        self.min_distance = float(\'inf\')\n        self.obstacle_angle = 0.0\n\n    def scan_callback(self, msg):\n        """Process laser scan for obstacles"""\n\n        # Find minimum distance\n        ranges = np.array(msg.ranges)\n        ranges[ranges == 0] = float(\'inf\')  # Ignore invalid\n\n        self.min_distance = np.min(ranges)\n        min_idx = np.argmin(ranges)\n\n        # Calculate angle to obstacle\n        self.obstacle_angle = msg.angle_min + min_idx * msg.angle_increment\n\n        # Generate avoidance command\n        self.avoid_obstacle()\n\n    def avoid_obstacle(self):\n        """Generate velocity to avoid obstacle"""\n        cmd = Twist()\n\n        if self.min_distance < 0.5:\n            # Too close, turn away\n            cmd.linear.x = 0.0\n            cmd.angular.z = -np.sign(self.obstacle_angle) * 1.0\n        elif self.min_distance < 1.0:\n            # Moderate distance, slow down and adjust\n            cmd.linear.x = 0.1\n            cmd.angular.z = -np.sign(self.obstacle_angle) * 0.5\n        else:\n            # Safe, proceed normally\n            cmd.linear.x = 0.3\n\n        self.cmd_vel_pub.publish(cmd)\n'})}),"\n",(0,r.jsx)(n.h2,{id:"performance-monitoring",children:"Performance Monitoring"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'class PerformanceMonitor(Node):\n    def __init__(self):\n        super().__init__(\'performance_monitor\')\n\n        self.detection_times = []\n        self.control_times = []\n\n        # Timers\n        self.last_detection_time = self.get_clock().now()\n\n    def log_detection_latency(self):\n        """Measure detection pipeline latency"""\n        now = self.get_clock().now()\n        latency = (now - self.last_detection_time).nanoseconds / 1e6  # ms\n\n        self.detection_times.append(latency)\n\n        if len(self.detection_times) > 100:\n            avg_latency = np.mean(self.detection_times)\n            self.get_logger().info(f"Avg detection latency: {avg_latency:.1f}ms")\n            self.detection_times = []\n\n        self.last_detection_time = now\n'})}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://control.ros.org/",children:"ros2_control"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://moveit.ros.org/",children:"MoveIt 2"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://navigation.ros.org/",children:"Navigation2"})}),"\n"]})]})}function _(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>a});var r=t(6540);const o={},s=r.createContext(o);function i(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);