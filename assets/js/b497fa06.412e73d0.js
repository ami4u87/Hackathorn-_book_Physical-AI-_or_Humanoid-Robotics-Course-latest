"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[576],{6085:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>l});var s=i(4848),t=i(8453);const o={},a="Handling Ambiguous Commands",r={id:"module-4-vla/ambiguity",title:"Handling Ambiguous Commands",description:"Resolve ambiguities in natural language commands through clarification and context.",source:"@site/docs/module-4-vla/5-ambiguity.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/ambiguity",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/ambiguity",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-4-vla/5-ambiguity.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{},sidebar:"docsSidebar",previous:{title:"Action Feasibility Validation",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/validation"},next:{title:"Module 4 Exercises",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/exercises"}},c={},l=[{value:"Types of Ambiguity",id:"types-of-ambiguity",level:2},{value:"Ambiguity Detection",id:"ambiguity-detection",level:2},{value:"Clarification Generation",id:"clarification-generation",level:2},{value:"Interactive Disambiguation",id:"interactive-disambiguation",level:2},{value:"Context-Based Resolution",id:"context-based-resolution",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Further Reading",id:"further-reading",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"handling-ambiguous-commands",children:"Handling Ambiguous Commands"}),"\n",(0,s.jsx)(n.p,{children:"Resolve ambiguities in natural language commands through clarification and context."}),"\n",(0,s.jsx)(n.h2,{id:"types-of-ambiguity",children:"Types of Ambiguity"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Ambiguity"}),': "Pick up the cup" (which cup?)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Location Ambiguity"}),': "Place it there" (where exactly?)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Method Ambiguity"}),': "Move the box" (push, carry, or drag?)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Temporal Ambiguity"}),': "Do it later" (when?)']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Goal Ambiguity"}),': "Clean up" (what constitutes clean?)']}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"ambiguity-detection",children:"Ambiguity Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"class AmbiguityDetector:\n    def detect_ambiguities(self, command, scene_description):\n        \"\"\"\n        Detect potential ambiguities in command\n\n        Returns:\n            List of detected ambiguities with types\n        \"\"\"\n        ambiguities = []\n\n        # Check for underspecified objects\n        if self.has_vague_reference(command):\n            ambiguities.append({\n                'type': 'object_reference',\n                'issue': 'Vague object reference detected',\n                'examples': ['the cup', 'it', 'that one']\n            })\n\n        # Check for missing location\n        if 'place' in command.lower() and not self.has_location(command):\n            ambiguities.append({\n                'type': 'location',\n                'issue': 'Target location not specified'\n            })\n\n        # Check for multiple viable interpretations\n        interpretations = self.count_interpretations(command, scene_description)\n        if interpretations > 1:\n            ambiguities.append({\n                'type': 'multiple_interpretations',\n                'issue': f'{interpretations} possible interpretations found'\n            })\n\n        return ambiguities\n\n    def has_vague_reference(self, command):\n        \"\"\"Check for vague object references\"\"\"\n        vague_terms = ['it', 'that', 'this', 'the thing', 'something']\n        return any(term in command.lower() for term in vague_terms)\n\n    def has_location(self, command):\n        \"\"\"Check if command includes location\"\"\"\n        location_terms = ['on', 'in', 'at', 'to', 'near', 'beside']\n        return any(term in command.lower() for term in location_terms)\n"})}),"\n",(0,s.jsx)(n.h2,{id:"clarification-generation",children:"Clarification Generation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ClarificationGenerator:\n    def __init__(self, vla_client):\n        self.vla = vla_client\n\n    def generate_clarification_questions(self, command, image, ambiguities):\n        """\n        Generate natural clarification questions\n\n        Returns:\n            List of questions to ask user\n        """\n        prompt = f"""\n        User command: "{command}"\n\n        Detected ambiguities:\n        {json.dumps(ambiguities, indent=2)}\n\n        Scene: [see image]\n\n        Generate 1-3 concise clarification questions to resolve these ambiguities.\n        Make questions specific to objects visible in the scene.\n\n        Format as JSON:\n        {{\n            "questions": [\n                {{"question": "...", "resolves": "object_reference"}},\n                ...\n            ]\n        }}\n        """\n\n        image_base64 = self.vla.encode_image(image)\n\n        response = self.vla.client.chat.completions.create(\n            model="gpt-4-vision-preview",\n            messages=[\n                {\n                    "role": "user",\n                    "content": [\n                        {"type": "text", "text": prompt},\n                        {\n                            "type": "image_url",\n                            "image_url": {"url": f"data:image/jpeg;base64,{image_base64}"}\n                        }\n                    ]\n                }\n            ],\n            response_format={"type": "json_object"}\n        )\n\n        result = json.loads(response.choices[0].message.content)\n        return result[\'questions\']\n'})}),"\n",(0,s.jsx)(n.h2,{id:"interactive-disambiguation",children:"Interactive Disambiguation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class InteractiveDisambiguator:\n    def __init__(self, vla_client):\n        self.vla = vla_client\n        self.detector = AmbiguityDetector()\n        self.clarifier = ClarificationGenerator(vla_client)\n\n    def resolve_command(self, command, image, interaction_callback):\n        """\n        Iteratively resolve ambiguities\n\n        Args:\n            command: Original command\n            image: Scene image\n            interaction_callback: Function to get user responses\n\n        Returns:\n            Resolved, unambiguous command\n        """\n        resolved_command = command\n        scene_desc = self.describe_scene(image)\n\n        max_iterations = 3\n        for iteration in range(max_iterations):\n            # Detect ambiguities\n            ambiguities = self.detector.detect_ambiguities(resolved_command, scene_desc)\n\n            if not ambiguities:\n                # No more ambiguities\n                return resolved_command\n\n            # Generate clarification questions\n            questions = self.clarifier.generate_clarification_questions(\n                resolved_command, image, ambiguities\n            )\n\n            # Get user responses\n            responses = interaction_callback(questions)\n\n            # Incorporate responses\n            resolved_command = self.incorporate_responses(\n                resolved_command, questions, responses\n            )\n\n        return resolved_command\n\n    def describe_scene(self, image):\n        """Get scene description for ambiguity detection"""\n        # Use VLA to describe scene\n        prompt = "Describe all objects in this scene briefly."\n        description = self.vla.query(image, prompt)\n        return description\n\n    def incorporate_responses(self, command, questions, responses):\n        """Update command with clarification responses"""\n        prompt = f"""\n        Original command: "{command}"\n\n        Clarification Q&A:\n        {self.format_qa_pairs(questions, responses)}\n\n        Rewrite the command to be completely unambiguous based on the clarifications.\n        Provide only the rewritten command.\n        """\n\n        response = self.vla.client.chat.completions.create(\n            model="gpt-4-turbo-preview",\n            messages=[{"role": "user", "content": prompt}]\n        )\n\n        return response.choices[0].message.content.strip()\n\n    def format_qa_pairs(self, questions, responses):\n        """Format questions and answers"""\n        pairs = []\n        for q, r in zip(questions, responses):\n            pairs.append(f"Q: {q[\'question\']}\\nA: {r}")\n        return "\\n\\n".join(pairs)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"context-based-resolution",children:"Context-Based Resolution"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ContextResolver:\n    def __init__(self):\n        self.scene_graph = {}\n        self.recent_actions = []\n\n    def resolve_with_context(self, command):\n        """Resolve using scene graph and history"""\n\n        # Resolve pronoun references\n        if \'it\' in command.lower():\n            last_object = self.get_last_mentioned_object()\n            command = command.replace(\'it\', last_object)\n\n        # Resolve spatial references\n        if \'there\' in command.lower():\n            last_location = self.get_last_location()\n            command = command.replace(\'there\', f\'at {last_location}\')\n\n        return command\n\n    def get_last_mentioned_object(self):\n        """Get most recently mentioned object"""\n        if self.recent_actions:\n            last_action = self.recent_actions[-1]\n            if \'object\' in last_action:\n                return last_action[\'object\']\n        return "the object"\n\n    def get_last_location(self):\n        """Get most recently referenced location"""\n        if self.recent_actions:\n            for action in reversed(self.recent_actions):\n                if \'location\' in action:\n                    return f"({action[\'location\'][0]:.2f}, {action[\'location\'][1]:.2f})"\n        return "(0.5, 0.5)"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from std_msgs.msg import String\nfrom std_srvs.srv import Trigger\n\nclass DisambiguationNode(Node):\n    def __init__(self):\n        super().__init__(\'disambiguation_node\')\n\n        self.disambiguator = InteractiveDisambiguator(vla_client)\n\n        # Subscribers\n        self.command_sub = self.create_subscription(\n            String, \'/voice_command\', self.command_callback, 10\n        )\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image\', self.image_callback, 10\n        )\n\n        # Publishers\n        self.clarification_pub = self.create_publisher(\n            String, \'/clarification_question\', 10\n        )\n        self.resolved_command_pub = self.create_publisher(\n            String, \'/resolved_command\', 10\n        )\n\n        # Service for user responses\n        self.response_srv = self.create_service(\n            Trigger, \'/user_response\', self.handle_user_response\n        )\n\n        self.latest_image = None\n        self.pending_questions = []\n        self.user_responses = []\n\n    def command_callback(self, msg):\n        """Handle incoming command"""\n        if self.latest_image is None:\n            self.get_logger().warn("No image available")\n            return\n\n        # Resolve with interaction\n        resolved = self.disambiguator.resolve_command(\n            msg.data,\n            self.latest_image,\n            self.interaction_callback\n        )\n\n        # Publish resolved command\n        resolved_msg = String()\n        resolved_msg.data = resolved\n        self.resolved_command_pub.publish(resolved_msg)\n\n    def interaction_callback(self, questions):\n        """Ask user for clarifications"""\n        self.pending_questions = questions\n        self.user_responses = []\n\n        # Publish questions\n        for q in questions:\n            msg = String()\n            msg.data = q[\'question\']\n            self.clarification_pub.publish(msg)\n\n        # Wait for responses (in practice, use async/await)\n        while len(self.user_responses) < len(questions):\n            time.sleep(0.1)\n\n        return self.user_responses\n\n    def handle_user_response(self, request, response):\n        """Handle user response to clarification"""\n        # Store response\n        self.user_responses.append(request.data)\n\n        response.success = True\n        return response\n'})}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2010.12083",children:"Natural Language Disambiguation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2203.00954",children:"Interactive Robot Learning"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(m,{...e})}):m(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var s=i(6540);const t={},o=s.createContext(t);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);