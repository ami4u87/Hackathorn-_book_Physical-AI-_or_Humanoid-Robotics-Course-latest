"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[581],{5610:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docsSidebar":[{"type":"link","label":"Physical AI & Humanoid Robotics","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/intro","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: ROS 2 Fundamentals","items":[{"type":"link","label":"Module 1: ROS 2 Fundamentals","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/","docId":"module-1-ros2/index","unlisted":false},{"type":"link","label":"Installation","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/installation","docId":"module-1-ros2/installation","unlisted":false},{"type":"link","label":"Publisher-Subscriber Pattern","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/pubsub","docId":"module-1-ros2/pubsub","unlisted":false},{"type":"link","label":"Services","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/services","docId":"module-1-ros2/services","unlisted":false},{"type":"link","label":"Actions","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/actions","docId":"module-1-ros2/actions","unlisted":false},{"type":"link","label":"Parameters","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/parameters","docId":"module-1-ros2/parameters","unlisted":false},{"type":"link","label":"Launch Files","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/launch","docId":"module-1-ros2/launch","unlisted":false},{"type":"link","label":"TF2 (Transform Library)","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/tf2","docId":"module-1-ros2/tf2","unlisted":false},{"type":"link","label":"Module 1 Exercises","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-1-ros2/exercises","docId":"module-1-ros2/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: Simulation Environments","items":[{"type":"link","label":"Module 2: Simulation Environments","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-2-simulation/","docId":"module-2-simulation/index","unlisted":false},{"type":"link","label":"Gazebo Harmonic with ROS 2","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-2-simulation/gazebo","docId":"module-2-simulation/gazebo","unlisted":false},{"type":"link","label":"Unity with ROS-TCP-Connector","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-2-simulation/unity","docId":"module-2-simulation/unity","unlisted":false},{"type":"link","label":"NVIDIA Isaac Sim","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-2-simulation/isaac","docId":"module-2-simulation/isaac","unlisted":false},{"type":"link","label":"Physics Validation","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-2-simulation/physics","docId":"module-2-simulation/physics","unlisted":false},{"type":"link","label":"Module 2 Exercises","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-2-simulation/exercises","docId":"module-2-simulation/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: Perception & Control","items":[{"type":"link","label":"Module 3: Perception & Control","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/","docId":"module-3-perception/index","unlisted":false},{"type":"link","label":"Vision and Depth Perception","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/vision-depth","docId":"module-3-perception/vision-depth","unlisted":false},{"type":"link","label":"Object Detection and Classification","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/object-detection","docId":"module-3-perception/object-detection","unlisted":false},{"type":"link","label":"Pose Estimation and TF Publishing","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/pose-tf","docId":"module-3-perception/pose-tf","unlisted":false},{"type":"link","label":"RViz2 Visualization","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/rviz","docId":"module-3-perception/rviz","unlisted":false},{"type":"link","label":"Control System Integration","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/control-integration","docId":"module-3-perception/control-integration","unlisted":false},{"type":"link","label":"Module 3 Exercises","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/exercises","docId":"module-3-perception/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action","items":[{"type":"link","label":"Module 4: Vision-Language-Action","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/","docId":"module-4-vla/index","unlisted":false},{"type":"link","label":"GPT-4 Vision Integration","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/gpt4-vision","docId":"module-4-vla/gpt4-vision","unlisted":false},{"type":"link","label":"Multi-Modal Processing","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/multimodal","docId":"module-4-vla/multimodal","unlisted":false},{"type":"link","label":"Structured Action Generation","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/action-sequences","docId":"module-4-vla/action-sequences","unlisted":false},{"type":"link","label":"Action Feasibility Validation","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/validation","docId":"module-4-vla/validation","unlisted":false},{"type":"link","label":"Handling Ambiguous Commands","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/ambiguity","docId":"module-4-vla/ambiguity","unlisted":false},{"type":"link","label":"Module 4 Exercises","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/exercises","docId":"module-4-vla/exercises","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 5: Capstone Project","items":[{"type":"link","label":"Module 5: Capstone Project","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-5-capstone/","docId":"module-5-capstone/index","unlisted":false},{"type":"link","label":"System Integration","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-5-capstone/integration","docId":"module-5-capstone/integration","unlisted":false},{"type":"link","label":"Voice Command Interface","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-5-capstone/voice-interface","docId":"module-5-capstone/voice-interface","unlisted":false},{"type":"link","label":"Final Demo and Presentation","href":"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-5-capstone/final-demo","docId":"module-5-capstone/final-demo","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"intro":{"id":"intro","title":"Physical AI & Humanoid Robotics","description":"Welcome to the comprehensive course on Physical AI and Humanoid Robotics. This course covers ROS 2 fundamentals, simulation environments, perception pipelines, vision-language-action systems, and culminates in a capstone project with voice-commanded autonomous manipulation.","sidebar":"docsSidebar"},"module-1-ros2/actions":{"id":"module-1-ros2/actions","title":"Actions","description":"Actions in ROS 2 provide a communication pattern for long-running tasks that require feedback, goal management, and cancellation capabilities. Unlike services, which are synchronous and blocking, actions allow for asynchronous execution with continuous feedback. This makes them ideal for tasks like robot navigation, manipulation, or any operation that takes a significant amount of time to complete.","sidebar":"docsSidebar"},"module-1-ros2/exercises":{"id":"module-1-ros2/exercises","title":"Module 1 Exercises","description":"This section contains hands-on exercises to reinforce the concepts learned in Module 1: ROS 2 Fundamentals. Each exercise includes prompts, acceptance criteria, and validation guidance to help you verify your understanding.","sidebar":"docsSidebar"},"module-1-ros2/index":{"id":"module-1-ros2/index","title":"Module 1: ROS 2 Fundamentals","description":"This module introduces the core concepts of ROS 2, including the publisher-subscriber pattern, services, actions, parameters, and launch files. By the end of this module, you will understand how to create basic ROS 2 nodes and communicate between them.","sidebar":"docsSidebar"},"module-1-ros2/installation":{"id":"module-1-ros2/installation","title":"Installation","description":"This section covers the installation of ROS 2 Humble Hawksbill on Ubuntu 22.04 LTS. ROS 2 (Robot Operating System 2) is a flexible framework for writing robot software that provides services such as hardware abstraction, device drivers, libraries, visualizers, message-passing, package management, and more.","sidebar":"docsSidebar"},"module-1-ros2/launch":{"id":"module-1-ros2/launch","title":"Launch Files","description":"Launch files in ROS 2 provide a way to start multiple nodes with specific configurations simultaneously. They allow you to define complex robotic systems with a single command, making it easy to reproduce experiments, switch between different configurations, and manage complex multi-node applications.","sidebar":"docsSidebar"},"module-1-ros2/parameters":{"id":"module-1-ros2/parameters","title":"Parameters","description":"Parameters in ROS 2 provide a way to configure nodes dynamically at runtime. They allow you to modify node behavior without recompiling code, making your robotic systems more flexible and easier to tune. Parameters can be set at launch time, changed during runtime, and even declared with specific types and constraints.","sidebar":"docsSidebar"},"module-1-ros2/pubsub":{"id":"module-1-ros2/pubsub","title":"Publisher-Subscriber Pattern","description":"The Publisher-Subscriber (Pub/Sub) pattern is one of the fundamental communication mechanisms in ROS 2. It enables asynchronous message passing between different nodes in a robotic system. Publishers send messages to topics, and subscribers receive messages from topics without needing to know about each other directly.","sidebar":"docsSidebar"},"module-1-ros2/services":{"id":"module-1-ros2/services","title":"Services","description":"Services in ROS 2 provide a request-response communication pattern that is different from the publisher-subscriber model. In service-based communication, a service client sends a request to a service server, which processes the request and sends back a response. This is useful for operations that require immediate feedback or when you need to ensure that a specific task is completed before proceeding.","sidebar":"docsSidebar"},"module-1-ros2/tf2":{"id":"module-1-ros2/tf2","title":"TF2 (Transform Library)","description":"TF2 (Transform Library 2) is a package in ROS 2 that provides a framework for tracking coordinate frames in a robotic system over time. It allows you to transform points, vectors, and other data between different coordinate frames, which is essential for tasks like navigation, manipulation, and sensor fusion in robotics.","sidebar":"docsSidebar"},"module-2-simulation/exercises":{"id":"module-2-simulation/exercises","title":"Module 2 Exercises","description":"These exercises will help you practice the simulation concepts covered in this module. Complete them in order, as they build on each other.","sidebar":"docsSidebar"},"module-2-simulation/gazebo":{"id":"module-2-simulation/gazebo","title":"Gazebo Harmonic with ROS 2","description":"Gazebo is a powerful 3D robotics simulator that provides accurate physics simulation and sensor modeling. In this lesson, you\'ll learn to set up Gazebo Harmonic (the latest version) with ROS 2 and simulate humanoid robots.","sidebar":"docsSidebar"},"module-2-simulation/index":{"id":"module-2-simulation/index","title":"Module 2: Simulation Environments","description":"This module covers simulation environments for humanoid robotics, including Gazebo, Unity, and NVIDIA Isaac. You will learn to set up simulation environments, configure robot models, and validate physics-based interactions.","sidebar":"docsSidebar"},"module-2-simulation/isaac":{"id":"module-2-simulation/isaac","title":"NVIDIA Isaac Sim","description":"NVIDIA Isaac Sim is a robotics simulation platform built on NVIDIA Omniverse, providing photorealistic, physically accurate virtual environments with GPU-accelerated computation. It\'s designed specifically for robotic applications with advanced features for AI and machine learning.","sidebar":"docsSidebar"},"module-2-simulation/physics":{"id":"module-2-simulation/physics","title":"Physics Validation","description":"Ensuring that your robot simulation accurately represents real-world physics is critical for successful sim-to-real transfer. This lesson covers techniques for validating and tuning physics parameters in your simulation environments.","sidebar":"docsSidebar"},"module-2-simulation/unity":{"id":"module-2-simulation/unity","title":"Unity with ROS-TCP-Connector","description":"Unity is a powerful game engine that can be used for high-fidelity robot simulation with photorealistic rendering and advanced physics. The ROS-TCP-Connector enables seamless communication between Unity and ROS 2.","sidebar":"docsSidebar"},"module-3-perception/control-integration":{"id":"module-3-perception/control-integration","title":"Control System Integration","description":"Integrating perception with control systems allows robots to react to their environment. This lesson covers perception-control loops, visual servoing, and reactive behaviors.","sidebar":"docsSidebar"},"module-3-perception/exercises":{"id":"module-3-perception/exercises","title":"Module 3 Exercises","description":"Practice perception and control integration with these hands-on exercises.","sidebar":"docsSidebar"},"module-3-perception/index":{"id":"module-3-perception/index","title":"Module 3: Perception & Control","description":"This module focuses on perception systems for robotics, including vision and depth perception, object detection, and pose estimation. You will learn to implement perception pipelines and integrate them with robot control systems.","sidebar":"docsSidebar"},"module-3-perception/object-detection":{"id":"module-3-perception/object-detection","title":"Object Detection and Classification","description":"Object detection enables robots to identify and locate objects in their environment. This lesson covers traditional computer vision methods and modern deep learning approaches for object detection.","sidebar":"docsSidebar"},"module-3-perception/pose-tf":{"id":"module-3-perception/pose-tf","title":"Pose Estimation and TF Publishing","description":"Pose estimation determines the 3D position and orientation of objects. Publishing these poses to the TF (Transform) tree allows the robot to reason about spatial relationships and plan actions.","sidebar":"docsSidebar"},"module-3-perception/rviz":{"id":"module-3-perception/rviz","title":"RViz2 Visualization","description":"RViz2 is the primary visualization tool for ROS 2, allowing you to see sensor data, robot state, and planning results in 3D.","sidebar":"docsSidebar"},"module-3-perception/vision-depth":{"id":"module-3-perception/vision-depth","title":"Vision and Depth Perception","description":"Vision is the primary sensing modality for humanoid robots to understand their environment. This lesson covers RGB cameras, depth sensors, and techniques for extracting meaningful information from visual data.","sidebar":"docsSidebar"},"module-4-vla/action-sequences":{"id":"module-4-vla/action-sequences","title":"Structured Action Generation","description":"Generate executable robot action sequences from high-level commands.","sidebar":"docsSidebar"},"module-4-vla/ambiguity":{"id":"module-4-vla/ambiguity","title":"Handling Ambiguous Commands","description":"Resolve ambiguities in natural language commands through clarification and context.","sidebar":"docsSidebar"},"module-4-vla/exercises":{"id":"module-4-vla/exercises","title":"Module 4 Exercises","description":"Practice Vision-Language-Action integration with these exercises.","sidebar":"docsSidebar"},"module-4-vla/gpt4-vision":{"id":"module-4-vla/gpt4-vision","title":"GPT-4 Vision Integration","description":"Vision-Language-Action (VLA) models combine visual perception with natural language understanding to generate robot actions. This lesson covers integrating GPT-4 Vision (GPT-4V) with robotic systems.","sidebar":"docsSidebar"},"module-4-vla/index":{"id":"module-4-vla/index","title":"Module 4: Vision-Language-Action","description":"This module introduces Vision-Language-Action (VLA) models for interpreting natural language commands and visual scenes, outputting high-level action plans for robotic systems.","sidebar":"docsSidebar"},"module-4-vla/multimodal":{"id":"module-4-vla/multimodal","title":"Multi-Modal Processing","description":"Process camera images and text together for enhanced robot understanding.","sidebar":"docsSidebar"},"module-4-vla/validation":{"id":"module-4-vla/validation","title":"Action Feasibility Validation","description":"Validate that generated actions are safe and executable before execution.","sidebar":"docsSidebar"},"module-5-capstone/final-demo":{"id":"module-5-capstone/final-demo","title":"Final Demo and Presentation","description":"Complete your capstone by demonstrating a fully integrated humanoid robot system.","sidebar":"docsSidebar"},"module-5-capstone/index":{"id":"module-5-capstone/index","title":"Module 5: Capstone Project","description":"This capstone module integrates all previous modules into a comprehensive demonstration where a simulated humanoid responds to voice commands, perceives the environment, plans motion, and executes manipulation tasks autonomously.","sidebar":"docsSidebar"},"module-5-capstone/integration":{"id":"module-5-capstone/integration","title":"System Integration","description":"The capstone project integrates all previous modules into a complete autonomous humanoid robot system.","sidebar":"docsSidebar"},"module-5-capstone/voice-interface":{"id":"module-5-capstone/voice-interface","title":"Voice Command Interface","description":"Enable natural voice interaction with your humanoid robot using speech-to-text and text-to-speech.","sidebar":"docsSidebar"}}}')}}]);