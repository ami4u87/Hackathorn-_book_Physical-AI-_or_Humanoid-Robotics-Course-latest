"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[26],{6045:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var i=a(4848),t=a(8453);const s={},o="GPT-4 Vision Integration",r={id:"module-4-vla/gpt4-vision",title:"GPT-4 Vision Integration",description:"Vision-Language-Action (VLA) models combine visual perception with natural language understanding to generate robot actions. This lesson covers integrating GPT-4 Vision (GPT-4V) with robotic systems.",source:"@site/docs/module-4-vla/1-gpt4-vision.md",sourceDirName:"module-4-vla",slug:"/module-4-vla/gpt4-vision",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/gpt4-vision",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-4-vla/1-gpt4-vision.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"docsSidebar",previous:{title:"Module 4: Vision-Language-Action",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/"},next:{title:"Multi-Modal Processing",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-4-vla/multimodal"}},c={},l=[{value:"What is VLA?",id:"what-is-vla",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Setting Up OpenAI API",id:"setting-up-openai-api",level:2},{value:"Basic Usage Example",id:"basic-usage-example",level:2},{value:"Structured Action Generation",id:"structured-action-generation",level:2},{value:"Example Usage",id:"example-usage",level:2},{value:"ROS 2 Integration",id:"ros-2-integration",level:2},{value:"Optimizations",id:"optimizations",level:2},{value:"1. Image Compression",id:"1-image-compression",level:3},{value:"2. Caching",id:"2-caching",level:3},{value:"3. Batch Processing",id:"3-batch-processing",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Further Reading",id:"further-reading",level:2}];function m(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"gpt-4-vision-integration",children:"GPT-4 Vision Integration"}),"\n",(0,i.jsx)(n.p,{children:"Vision-Language-Action (VLA) models combine visual perception with natural language understanding to generate robot actions. This lesson covers integrating GPT-4 Vision (GPT-4V) with robotic systems."}),"\n",(0,i.jsx)(n.h2,{id:"what-is-vla",children:"What is VLA?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Vision-Language-Action"})," models:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision"}),": Process camera images"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Language"}),": Understand natural language commands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action"}),": Generate executable robot actions"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Input: Image + "Pick up the red cup"'}),"\n",(0,i.jsxs)(n.li,{children:["Output: ",(0,i.jsx)(n.code,{children:'grasp(object="red_cup", position=[0.5, 0.2, 0.1])'})]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Camera \u2192 Image \u2192 GPT-4V \u2192 Structured \u2192 Motion \u2192 Robot\nCommand \u2192 Text \u2192         \u2192 Actions  \u2192 Planning \u2192 Execution\n"})}),"\n",(0,i.jsx)(n.h2,{id:"setting-up-openai-api",children:"Setting Up OpenAI API"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Install OpenAI SDK\npip install openai\n\n# Set API key\nexport OPENAI_API_KEY="your-api-key-here"\n'})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\nimport base64\nimport cv2\n\nclass GPT4VisionClient:\n    def __init__(self, api_key=None):\n        self.client = OpenAI(api_key=api_key)\n        self.model = "gpt-4-vision-preview"\n\n    def encode_image(self, image):\n        """\n        Encode OpenCV image to base64\n\n        Args:\n            image: OpenCV image (BGR)\n\n        Returns:\n            base64 string\n        """\n        # Convert to RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Encode to JPEG\n        success, buffer = cv2.imencode(\'.jpg\', image_rgb)\n\n        if not success:\n            raise ValueError("Failed to encode image")\n\n        # Convert to base64\n        image_base64 = base64.b64encode(buffer).decode(\'utf-8\')\n\n        return image_base64\n\n    def query(self, image, prompt, max_tokens=500):\n        """\n        Query GPT-4V with image and text prompt\n\n        Args:\n            image: OpenCV image or base64 string\n            prompt: Text prompt/question\n            max_tokens: Maximum response length\n\n        Returns:\n            Response text\n        """\n        # Encode image if needed\n        if isinstance(image, str):\n            image_data = image\n        else:\n            image_data = self.encode_image(image)\n\n        # Create message\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=[\n                {\n                    "role": "user",\n                    "content": [\n                        {\n                            "type": "text",\n                            "text": prompt\n                        },\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{image_data}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            max_tokens=max_tokens\n        )\n\n        return response.choices[0].message.content\n'})}),"\n",(0,i.jsx)(n.h2,{id:"basic-usage-example",children:"Basic Usage Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Initialize client\nclient = GPT4VisionClient()\n\n# Load image\nimage = cv2.imread(\'robot_scene.jpg\')\n\n# Query\nprompt = "Describe all objects you see in this image."\nresponse = client.query(image, prompt)\n\nprint(response)\n# Output: "I see a table with a red cup, blue book, and yellow banana..."\n'})}),"\n",(0,i.jsx)(n.h2,{id:"structured-action-generation",children:"Structured Action Generation"}),"\n",(0,i.jsx)(n.p,{children:"Use function calling for structured outputs."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import json\n\nclass VLAActionGenerator:\n    def __init__(self, api_key=None):\n        self.client = OpenAI(api_key=api_key)\n\n        # Define available robot actions\n        self.actions_schema = {\n            "name": "generate_robot_actions",\n            "description": "Generate a sequence of robot actions based on visual scene and command",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "actions": {\n                        "type": "array",\n                        "items": {\n                            "type": "object",\n                            "properties": {\n                                "action_type": {\n                                    "type": "string",\n                                    "enum": ["grasp", "place", "move_to", "rotate", "wait"]\n                                },\n                                "object_name": {\n                                    "type": "string"\n                                },\n                                "position": {\n                                    "type": "array",\n                                    "items": {"type": "number"},\n                                    "minItems": 3,\n                                    "maxItems": 3\n                                },\n                                "gripper_state": {\n                                    "type": "string",\n                                    "enum": ["open", "close"]\n                                }\n                            },\n                            "required": ["action_type"]\n                        }\n                    }\n                },\n                "required": ["actions"]\n            }\n        }\n\n    def generate_actions(self, image, command):\n        """\n        Generate structured action sequence\n\n        Args:\n            image: OpenCV image\n            command: Natural language command\n\n        Returns:\n            List of action dictionaries\n        """\n        # Encode image\n        image_base64 = self.encode_image(image)\n\n        # Create prompt\n        prompt = f"""\n        You are a robot action planner. Given the scene in the image and the command: "{command}",\n        generate a sequence of robot actions.\n\n        Available actions:\n        - grasp: Pick up an object\n        - place: Put down an object\n        - move_to: Move to a position\n        - rotate: Rotate gripper\n        - wait: Pause execution\n\n        Provide actions as a structured JSON sequence.\n        """\n\n        response = self.client.chat.completions.create(\n            model="gpt-4-vision-preview",\n            messages=[\n                {\n                    "role": "user",\n                    "content": [\n                        {"type": "text", "text": prompt},\n                        {\n                            "type": "image_url",\n                            "image_url": {\n                                "url": f"data:image/jpeg;base64,{image_base64}"\n                            }\n                        }\n                    ]\n                }\n            ],\n            functions=[self.actions_schema],\n            function_call={"name": "generate_robot_actions"}\n        )\n\n        # Extract actions\n        function_call = response.choices[0].message.function_call\n        actions = json.loads(function_call.arguments)\n\n        return actions[\'actions\']\n\n    def encode_image(self, image):\n        """Encode OpenCV image to base64"""\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        success, buffer = cv2.imencode(\'.jpg\', image_rgb)\n        return base64.b64encode(buffer).decode(\'utf-8\')\n'})}),"\n",(0,i.jsx)(n.h2,{id:"example-usage",children:"Example Usage"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Initialize\nvla = VLAActionGenerator()\n\n# Load scene image\nscene = cv2.imread('table_scene.jpg')\n\n# Generate actions\ncommand = \"Pick up the red cup and place it on the blue plate\"\nactions = vla.generate_actions(scene, command)\n\nprint(json.dumps(actions, indent=2))\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\n  {\n    "action_type": "move_to",\n    "position": [0.5, 0.2, 0.3],\n    "object_name": "red_cup"\n  },\n  {\n    "action_type": "grasp",\n    "object_name": "red_cup",\n    "gripper_state": "close"\n  },\n  {\n    "action_type": "move_to",\n    "position": [0.3, 0.4, 0.3],\n    "object_name": "blue_plate"\n  },\n  {\n    "action_type": "place",\n    "object_name": "red_cup",\n    "gripper_state": "open"\n  }\n]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"ros-2-integration",children:"ROS 2 Integration"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nfrom std_msgs.msg import String\n\nclass VLANode(Node):\n    def __init__(self):\n        super().__init__(\'vla_node\')\n\n        # VLA client\n        self.vla = VLAActionGenerator()\n\n        # CV Bridge\n        self.bridge = CvBridge()\n\n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image, \'/camera/image_raw\', self.image_callback, 10\n        )\n        self.command_sub = self.create_subscription(\n            String, \'/voice_command\', self.command_callback, 10\n        )\n\n        # Publisher\n        self.action_pub = self.create_publisher(\n            String, \'/robot_actions\', 10\n        )\n\n        # State\n        self.latest_image = None\n        self.processing = False\n\n    def image_callback(self, msg):\n        """Store latest image"""\n        self.latest_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n\n    def command_callback(self, msg):\n        """Process voice command"""\n        if self.processing:\n            self.get_logger().warn("Already processing a command")\n            return\n\n        if self.latest_image is None:\n            self.get_logger().warn("No image available")\n            return\n\n        self.processing = True\n\n        # Generate actions\n        command = msg.data\n        self.get_logger().info(f"Processing command: {command}")\n\n        try:\n            actions = self.vla.generate_actions(self.latest_image, command)\n\n            # Publish actions\n            action_msg = String()\n            action_msg.data = json.dumps(actions)\n            self.action_pub.publish(action_msg)\n\n            self.get_logger().info(f"Generated {len(actions)} actions")\n\n        except Exception as e:\n            self.get_logger().error(f"Action generation failed: {e}")\n\n        self.processing = False\n'})}),"\n",(0,i.jsx)(n.h2,{id:"optimizations",children:"Optimizations"}),"\n",(0,i.jsx)(n.h3,{id:"1-image-compression",children:"1. Image Compression"}),"\n",(0,i.jsx)(n.p,{children:"Reduce API costs by compressing images."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def compress_image(image, max_size=(640, 480), quality=85):\n    """\n    Compress image for API\n\n    Args:\n        image: OpenCV image\n        max_size: Maximum (width, height)\n        quality: JPEG quality (0-100)\n\n    Returns:\n        Compressed image\n    """\n    h, w = image.shape[:2]\n    max_w, max_h = max_size\n\n    # Resize if needed\n    if w > max_w or h > max_h:\n        scale = min(max_w / w, max_h / h)\n        new_w = int(w * scale)\n        new_h = int(h * scale)\n        image = cv2.resize(image, (new_w, new_h))\n\n    # Encode with quality\n    encode_param = [cv2.IMWRITE_JPEG_QUALITY, quality]\n    success, buffer = cv2.imencode(\'.jpg\', image, encode_param)\n\n    return cv2.imdecode(buffer, cv2.IMREAD_COLOR)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-caching",children:"2. Caching"}),"\n",(0,i.jsx)(n.p,{children:"Cache responses for similar queries."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import hashlib\n\nclass CachedVLA:\n    def __init__(self, vla_generator):\n        self.vla = vla_generator\n        self.cache = {}\n\n    def generate_actions(self, image, command):\n        """Generate actions with caching"""\n\n        # Create cache key\n        image_hash = hashlib.md5(image.tobytes()).hexdigest()\n        cache_key = f"{image_hash}_{command}"\n\n        if cache_key in self.cache:\n            return self.cache[cache_key]\n\n        # Generate\n        actions = self.vla.generate_actions(image, command)\n\n        # Cache\n        self.cache[cache_key] = actions\n\n        return actions\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-batch-processing",children:"3. Batch Processing"}),"\n",(0,i.jsx)(n.p,{children:"Process multiple queries efficiently."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'async def batch_generate_actions(vla, image_command_pairs):\n    """\n    Generate actions for multiple (image, command) pairs\n\n    Args:\n        vla: VLAActionGenerator instance\n        image_command_pairs: List of (image, command) tuples\n\n    Returns:\n        List of action sequences\n    """\n    import asyncio\n\n    tasks = [\n        asyncio.to_thread(vla.generate_actions, img, cmd)\n        for img, cmd in image_command_pairs\n    ]\n\n    results = await asyncio.gather(*tasks)\n\n    return results\n'})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def robust_vla_query(vla, image, command, max_retries=3):\n    """VLA query with retries and error handling"""\n\n    for attempt in range(max_retries):\n        try:\n            actions = vla.generate_actions(image, command)\n            return actions\n\n        except Exception as e:\n            if attempt < max_retries - 1:\n                wait_time = 2 ** attempt  # Exponential backoff\n                print(f"Retry {attempt + 1} after {wait_time}s...")\n                time.sleep(wait_time)\n            else:\n                print(f"Failed after {max_retries} attempts: {e}")\n                return []\n\n    return []\n'})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsx)(n.p,{children:"In the next lesson, you'll learn to process multi-modal inputs (images + text) and generate more sophisticated action sequences."}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/vision",children:"OpenAI Vision API"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"Function Calling"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://cdn.openai.com/papers/GPTV_System_Card.pdf",children:"GPT-4V System Card"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>r});var i=a(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);