"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[655],{1539:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>a,toc:()=>d});var s=i(4848),r=i(8453);const t={},o="Vision and Depth Perception",a={id:"module-3-perception/vision-depth",title:"Vision and Depth Perception",description:"Vision is the primary sensing modality for humanoid robots to understand their environment. This lesson covers RGB cameras, depth sensors, and techniques for extracting meaningful information from visual data.",source:"@site/docs/module-3-perception/1-vision-depth.md",sourceDirName:"module-3-perception",slug:"/module-3-perception/vision-depth",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/vision-depth",draft:!1,unlisted:!1,editUrl:"https://github.com/ami4u87/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/tree/master/docs/module-3-perception/1-vision-depth.md",tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"docsSidebar",previous:{title:"Module 3: Perception & Control",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/"},next:{title:"Object Detection and Classification",permalink:"/Hackathorn-_book_Physical-AI-_or_Humanoid-Robotics-Course-latest/docs/module-3-perception/object-detection"}},l={},d=[{value:"Types of Vision Sensors",id:"types-of-vision-sensors",level:2},{value:"1. RGB Cameras",id:"1-rgb-cameras",level:3},{value:"2. Depth Cameras",id:"2-depth-cameras",level:3},{value:"3. RGB-D (RGB + Depth)",id:"3-rgb-d-rgb--depth",level:3},{value:"Camera Calibration",id:"camera-calibration",level:2},{value:"Intrinsic Parameters",id:"intrinsic-parameters",level:3},{value:"Calibration with ROS 2",id:"calibration-with-ros-2",level:3},{value:"Point Cloud Generation",id:"point-cloud-generation",level:2},{value:"Filtering and Processing",id:"filtering-and-processing",level:2},{value:"1. Noise Filtering",id:"1-noise-filtering",level:3},{value:"2. Downsampling",id:"2-downsampling",level:3},{value:"3. Plane Segmentation",id:"3-plane-segmentation",level:3},{value:"Stereo Vision",id:"stereo-vision",level:2},{value:"Visualization Tools",id:"visualization-tools",level:2},{value:"RViz2 Configuration",id:"rviz2-configuration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Region of Interest (ROI)",id:"1-region-of-interest-roi",level:3},{value:"2. Multi-threading",id:"2-multi-threading",level:3},{value:"Next Steps",id:"next-steps",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"vision-and-depth-perception",children:"Vision and Depth Perception"}),"\n",(0,s.jsx)(n.p,{children:"Vision is the primary sensing modality for humanoid robots to understand their environment. This lesson covers RGB cameras, depth sensors, and techniques for extracting meaningful information from visual data."}),"\n",(0,s.jsx)(n.h2,{id:"types-of-vision-sensors",children:"Types of Vision Sensors"}),"\n",(0,s.jsx)(n.h3,{id:"1-rgb-cameras",children:"1. RGB Cameras"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Characteristics"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Captures color information (Red, Green, Blue channels)"}),"\n",(0,s.jsx)(n.li,{children:"High resolution (640\xd7480 to 4K+)"}),"\n",(0,s.jsx)(n.li,{children:"Low cost"}),"\n",(0,s.jsx)(n.li,{children:"No direct depth information"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use Cases"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Object recognition"}),"\n",(0,s.jsx)(n.li,{children:"Color-based tracking"}),"\n",(0,s.jsx)(n.li,{children:"Visual servoing"}),"\n",(0,s.jsx)(n.li,{children:"Human detection"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Integration"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass CameraSubscriber(Node):\n    def __init__(self):\n        super().__init__('camera_subscriber')\n        self.subscription = self.create_subscription(\n            Image,\n            '/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS Image to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Process image\n        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n\n        # Display\n        cv2.imshow(\"Camera Feed\", cv_image)\n        cv2.waitKey(1)\n\ndef main():\n    rclpy.init()\n    node = CameraSubscriber()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"2-depth-cameras",children:"2. Depth Cameras"}),"\n",(0,s.jsx)(n.p,{children:"Depth cameras provide distance measurements for each pixel."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Technologies"}),":"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Technology"}),(0,s.jsx)(n.th,{children:"Range"}),(0,s.jsx)(n.th,{children:"Pros"}),(0,s.jsx)(n.th,{children:"Cons"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Stereo"})}),(0,s.jsx)(n.td,{children:"Triangulation"}),(0,s.jsx)(n.td,{children:"1-20m"}),(0,s.jsx)(n.td,{children:"Works outdoors"}),(0,s.jsx)(n.td,{children:"Needs texture"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Structured Light"})}),(0,s.jsx)(n.td,{children:"IR pattern"}),(0,s.jsx)(n.td,{children:"0.5-5m"}),(0,s.jsx)(n.td,{children:"High accuracy"}),(0,s.jsx)(n.td,{children:"Indoor only"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Time-of-Flight"})}),(0,s.jsx)(n.td,{children:"Light travel time"}),(0,s.jsx)(n.td,{children:"0.1-10m"}),(0,s.jsx)(n.td,{children:"Fast, accurate"}),(0,s.jsx)(n.td,{children:"Limited range"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"LiDAR"})}),(0,s.jsx)(n.td,{children:"Laser scanning"}),(0,s.jsx)(n.td,{children:"1-100m+"}),(0,s.jsx)(n.td,{children:"Long range"}),(0,s.jsx)(n.td,{children:"Expensive"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Popular Sensors"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Intel RealSense D435i"}),": Stereo + IMU, $200-400"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Azure Kinect"}),": ToF + RGB, $400"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ZED 2"}),": Stereo with AI, $450"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ouster OS1"}),": LiDAR, $3500+"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-rgb-d-rgb--depth",children:"3. RGB-D (RGB + Depth)"}),"\n",(0,s.jsx)(n.p,{children:"Combines color and depth in aligned frames."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 Depth Processing"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom sensor_msgs.msg import Image\nimport cv2\nfrom cv_bridge import CvBridge\n\nclass DepthProcessor(Node):\n    def __init__(self):\n        super().__init__('depth_processor')\n\n        # Subscribe to RGB and Depth\n        self.rgb_sub = self.create_subscription(\n            Image, '/camera/color/image_raw', self.rgb_callback, 10\n        )\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth/image_raw', self.depth_callback, 10\n        )\n\n        self.bridge = CvBridge()\n        self.rgb_image = None\n        self.depth_image = None\n\n    def rgb_callback(self, msg):\n        self.rgb_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n\n    def depth_callback(self, msg):\n        # Depth is typically in millimeters (uint16) or meters (float32)\n        self.depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n\n        if self.depth_image is not None:\n            self.process_depth()\n\n    def process_depth(self):\n        # Convert to meters if in millimeters\n        if self.depth_image.dtype == np.uint16:\n            depth_m = self.depth_image.astype(np.float32) / 1000.0\n        else:\n            depth_m = self.depth_image\n\n        # Filter invalid depths\n        depth_m[depth_m == 0] = np.nan\n        depth_m[depth_m > 10.0] = np.nan\n\n        # Visualize depth as colormap\n        depth_normalized = cv2.normalize(depth_m, None, 0, 255, cv2.NORM_MINMAX)\n        depth_normalized = np.nan_to_num(depth_normalized).astype(np.uint8)\n        depth_colored = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)\n\n        cv2.imshow(\"Depth\", depth_colored)\n        cv2.waitKey(1)\n\n    def get_distance_at_pixel(self, x, y):\n        \"\"\"Get distance to object at pixel (x, y)\"\"\"\n        if self.depth_image is None:\n            return None\n\n        depth_mm = self.depth_image[y, x]\n        depth_m = depth_mm / 1000.0\n\n        return depth_m\n"})}),"\n",(0,s.jsx)(n.h2,{id:"camera-calibration",children:"Camera Calibration"}),"\n",(0,s.jsx)(n.p,{children:"Camera calibration determines intrinsic parameters (focal length, principal point, distortion) needed for accurate measurements."}),"\n",(0,s.jsx)(n.h3,{id:"intrinsic-parameters",children:"Intrinsic Parameters"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Camera intrinsic matrix K\n# [fx  0  cx]\n# [ 0 fy  cy]\n# [ 0  0   1]\n\n# fx, fy: focal length in pixels\n# cx, cy: principal point (image center)\n\nclass CameraIntrinsics:\n    def __init__(self, fx, fy, cx, cy):\n        self.fx = fx\n        self.fy = fy\n        self.cx = cx\n        self.cy = cy\n\n        self.K = np.array([\n            [fx, 0,  cx],\n            [0,  fy, cy],\n            [0,  0,  1]\n        ])\n\n    def pixel_to_ray(self, u, v):\n        """Convert pixel coordinates to 3D ray direction"""\n        x = (u - self.cx) / self.fx\n        y = (v - self.cy) / self.fy\n        z = 1.0\n\n        ray = np.array([x, y, z])\n        ray = ray / np.linalg.norm(ray)  # Normalize\n\n        return ray\n\n    def project_point(self, point_3d):\n        """Project 3D point to pixel coordinates"""\n        x, y, z = point_3d\n\n        u = self.fx * (x / z) + self.cx\n        v = self.fy * (y / z) + self.cy\n\n        return int(u), int(v)\n\n# Example: RealSense D435i intrinsics (typical)\ncamera = CameraIntrinsics(\n    fx=615.0,  # pixels\n    fy=615.0,\n    cx=320.0,  # half of 640\n    cy=240.0   # half of 480\n)\n'})}),"\n",(0,s.jsx)(n.h3,{id:"calibration-with-ros-2",children:"Calibration with ROS 2"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Install calibration tools\nsudo apt install ros-humble-camera-calibration\n\n# Run calibration (print a checkerboard pattern)\nros2 run camera_calibration cameracalibrator \\\n    --size 8x6 \\\n    --square 0.024 \\\n    --ros-args -r image:=/camera/image_raw\n\n# After calibration, save to:\n# ~/.ros/camera_info/camera_name.yaml\n"})}),"\n",(0,s.jsx)(n.h2,{id:"point-cloud-generation",children:"Point Cloud Generation"}),"\n",(0,s.jsx)(n.p,{children:"Convert depth images to 3D point clouds."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import numpy as np\nfrom sensor_msgs.msg import PointCloud2, PointField\nimport struct\n\nclass PointCloudGenerator(Node):\n    def __init__(self):\n        super().__init__('point_cloud_generator')\n\n        self.depth_sub = self.create_subscription(\n            Image, '/camera/depth/image_raw', self.depth_callback, 10\n        )\n\n        self.pc_pub = self.create_publisher(\n            PointCloud2, '/camera/points', 10\n        )\n\n        # Camera intrinsics (load from camera_info in practice)\n        self.fx = 615.0\n        self.fy = 615.0\n        self.cx = 320.0\n        self.cy = 240.0\n\n        self.bridge = CvBridge()\n\n    def depth_callback(self, msg):\n        depth_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='16UC1')\n        depth_m = depth_image.astype(np.float32) / 1000.0\n\n        # Generate point cloud\n        points = self.depth_to_points(depth_m)\n\n        # Publish\n        pc_msg = self.create_point_cloud_msg(points, msg.header)\n        self.pc_pub.publish(pc_msg)\n\n    def depth_to_points(self, depth_image):\n        \"\"\"Convert depth image to 3D points\"\"\"\n        height, width = depth_image.shape\n        points = []\n\n        for v in range(0, height, 4):  # Subsample for efficiency\n            for u in range(0, width, 4):\n                z = depth_image[v, u]\n\n                if z == 0 or z > 10.0:  # Skip invalid\n                    continue\n\n                # Backproject to 3D\n                x = (u - self.cx) * z / self.fx\n                y = (v - self.cy) * z / self.fy\n\n                points.append([x, y, z])\n\n        return np.array(points)\n\n    def create_point_cloud_msg(self, points, header):\n        \"\"\"Create PointCloud2 message\"\"\"\n        msg = PointCloud2()\n        msg.header = header\n        msg.header.frame_id = \"camera_depth_optical_frame\"\n\n        msg.height = 1\n        msg.width = len(points)\n\n        msg.fields = [\n            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),\n            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),\n            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),\n        ]\n\n        msg.is_bigendian = False\n        msg.point_step = 12  # 3 floats \xd7 4 bytes\n        msg.row_step = msg.point_step * msg.width\n        msg.is_dense = True\n\n        # Pack data\n        buffer = []\n        for point in points:\n            buffer.append(struct.pack('fff', point[0], point[1], point[2]))\n\n        msg.data = b''.join(buffer)\n\n        return msg\n"})}),"\n",(0,s.jsx)(n.p,{children:"Visualize in RViz2:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 run rviz2 rviz2\n# Add > PointCloud2 > Topic: /camera/points\n"})}),"\n",(0,s.jsx)(n.h2,{id:"filtering-and-processing",children:"Filtering and Processing"}),"\n",(0,s.jsx)(n.h3,{id:"1-noise-filtering",children:"1. Noise Filtering"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import scipy.ndimage as ndimage\n\ndef filter_depth_image(depth_image):\n    """Remove noise from depth image"""\n\n    # 1. Remove outliers (isolated pixels)\n    depth_filtered = ndimage.median_filter(depth_image, size=5)\n\n    # 2. Fill small holes\n    mask = depth_filtered > 0\n    depth_filtered = ndimage.morphology.binary_closing(mask, iterations=2) * depth_filtered\n\n    # 3. Bilateral filter (edge-preserving)\n    depth_filtered = cv2.bilateralFilter(\n        depth_filtered.astype(np.float32),\n        d=9,\n        sigmaColor=75,\n        sigmaSpace=75\n    )\n\n    return depth_filtered\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-downsampling",children:"2. Downsampling"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def downsample_point_cloud(points, voxel_size=0.01):\n    """\n    Voxel grid downsampling\n\n    Args:\n        points: Nx3 array\n        voxel_size: Size of voxel in meters\n    """\n    # Quantize points to voxels\n    voxel_indices = np.floor(points / voxel_size).astype(int)\n\n    # Remove duplicates (keep one point per voxel)\n    _, unique_indices = np.unique(voxel_indices, axis=0, return_index=True)\n\n    downsampled = points[unique_indices]\n\n    return downsampled\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-plane-segmentation",children:"3. Plane Segmentation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def segment_ground_plane(points, distance_threshold=0.01, iterations=100):\n    """\n    RANSAC plane fitting\n\n    Args:\n        points: Nx3 point cloud\n        distance_threshold: Max distance to plane\n        iterations: RANSAC iterations\n\n    Returns:\n        inliers: Boolean mask of ground points\n        plane_model: [a, b, c, d] where ax + by + cz + d = 0\n    """\n    best_inliers = None\n    best_plane = None\n    best_count = 0\n\n    for _ in range(iterations):\n        # Sample 3 random points\n        sample_indices = np.random.choice(len(points), 3, replace=False)\n        p1, p2, p3 = points[sample_indices]\n\n        # Compute plane normal\n        v1 = p2 - p1\n        v2 = p3 - p1\n        normal = np.cross(v1, v2)\n        normal = normal / np.linalg.norm(normal)\n\n        # Plane equation: ax + by + cz + d = 0\n        a, b, c = normal\n        d = -np.dot(normal, p1)\n\n        # Compute distances\n        distances = np.abs(points @ normal + d)\n\n        # Count inliers\n        inliers = distances < distance_threshold\n        count = np.sum(inliers)\n\n        if count > best_count:\n            best_count = count\n            best_inliers = inliers\n            best_plane = np.array([a, b, c, d])\n\n    return best_inliers, best_plane\n'})}),"\n",(0,s.jsx)(n.h2,{id:"stereo-vision",children:"Stereo Vision"}),"\n",(0,s.jsx)(n.p,{children:"For cameras without built-in depth sensors, use stereo vision."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class StereoMatcher:\n    def __init__(self, baseline_m, focal_length_px):\n        """\n        Args:\n            baseline_m: Distance between cameras in meters\n            focal_length_px: Focal length in pixels\n        """\n        self.baseline = baseline_m\n        self.focal_length = focal_length_px\n\n        # Create stereo matcher (SGBM = Semi-Global Block Matching)\n        self.matcher = cv2.StereoSGBM_create(\n            minDisparity=0,\n            numDisparities=16*10,  # Must be divisible by 16\n            blockSize=5,\n            P1=8 * 3 * 5**2,\n            P2=32 * 3 * 5**2,\n            disp12MaxDiff=1,\n            uniquenessRatio=10,\n            speckleWindowSize=100,\n            speckleRange=32\n        )\n\n    def compute_disparity(self, left_image, right_image):\n        """Compute disparity map"""\n\n        # Convert to grayscale\n        left_gray = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)\n        right_gray = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)\n\n        # Compute disparity\n        disparity = self.matcher.compute(left_gray, right_gray).astype(np.float32) / 16.0\n\n        return disparity\n\n    def disparity_to_depth(self, disparity):\n        """Convert disparity to depth using: Z = (f \xd7 B) / d"""\n\n        # Avoid division by zero\n        disparity[disparity <= 0] = 0.1\n\n        depth = (self.focal_length * self.baseline) / disparity\n\n        return depth\n'})}),"\n",(0,s.jsx)(n.h2,{id:"visualization-tools",children:"Visualization Tools"}),"\n",(0,s.jsx)(n.h3,{id:"rviz2-configuration",children:"RViz2 Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Publish markers for visualization\nfrom visualization_msgs.msg import Marker\n\ndef publish_detection_marker(publisher, position, color):\n    """Publish a sphere marker at detected object position"""\n\n    marker = Marker()\n    marker.header.frame_id = "camera_link"\n    marker.header.stamp = self.get_clock().now().to_msg()\n\n    marker.type = Marker.SPHERE\n    marker.action = Marker.ADD\n\n    marker.pose.position.x = position[0]\n    marker.pose.position.y = position[1]\n    marker.pose.position.z = position[2]\n\n    marker.scale.x = 0.1\n    marker.scale.y = 0.1\n    marker.scale.z = 0.1\n\n    marker.color.r = color[0]\n    marker.color.g = color[1]\n    marker.color.b = color[2]\n    marker.color.a = 1.0\n\n    marker.lifetime.sec = 1\n\n    publisher.publish(marker)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"1-region-of-interest-roi",children:"1. Region of Interest (ROI)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def process_roi(image, roi_rect):\n    """Process only region of interest"""\n    x, y, w, h = roi_rect\n    roi = image[y:y+h, x:x+w]\n\n    # Process only ROI\n    processed_roi = expensive_operation(roi)\n\n    # Insert back\n    result = image.copy()\n    result[y:y+h, x:x+w] = processed_roi\n\n    return result\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-multi-threading",children:"2. Multi-threading"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import threading\n\nclass AsyncImageProcessor:\n    def __init__(self):\n        self.latest_image = None\n        self.lock = threading.Lock()\n        self.processing = False\n\n    def image_callback(self, msg):\n        with self.lock:\n            self.latest_image = self.bridge.imgmsg_to_cv2(msg)\n\n        # Process in separate thread\n        if not self.processing:\n            thread = threading.Thread(target=self.process_async)\n            thread.start()\n\n    def process_async(self):\n        self.processing = True\n\n        with self.lock:\n            image = self.latest_image.copy()\n\n        # Expensive processing\n        result = self.heavy_computation(image)\n\n        self.processing = False\n"})}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"In the next lesson, you'll learn object detection and classification using these vision inputs."}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://docs.opencv.org/",children:"OpenCV Documentation"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/ros-perception/image_pipeline",children:"ROS 2 Image Pipeline"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://pointclouds.org/",children:"Point Cloud Library"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/IntelRealSense/librealsense",children:"Intel RealSense SDK"})}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);